{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNESZxNTGvLg",
        "outputId": "f4af929f-05d3-4085-e240-a7c7b56c6488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ADAM_EPS = \"adam_eps\"\n",
        "ADAM_OPT = 'adam'\n",
        "BETA_ONE = \"beta_one\"\n",
        "BETA_TWO = \"beta_two\"\n",
        "BIDIRECTIONAL = \"bideractional\"\n",
        "DROP_WORD = \"drop_word\"\n",
        "HIDDEN_DIM = \"hidden_dim\"\n",
        "MAX_TRAIN_LENGTH = \"max_train_length\"\n",
        "MINI_BATCH_SIZE = \"mini_batch_size\"\n",
        "NUM_LAYERS = \"num_layers\"\n",
        "ADADELATA_OPT = 'adadelta'\n",
        "ADAM_OPT = 'adam'\n",
        "BATCH_SIZE = \"batch_size\"\n",
        "BCE_LOSS = 'bce_loss'\n",
        "CNN_KERNELS = \"cnn_kernels\"\n",
        "CNN_OUT_CHANNELS = \"cnn_out_channels\"\n",
        "CROSS_ENTROP_LOSS = 'cross_entropy_loss'\n",
        "DROPOUT = \"dropout\"\n",
        "EARLY_STOP_MAX_NO_IMP = \"early_stop_max_no_imp\"\n",
        "EMBED_WORDS_FILE = \"embed_words_file\"\n",
        "FREEZE_EMBEDDING = \"freeze_embeding\"\n",
        "LABEL_COL = \"label_col\"\n",
        "LEARNING_RATE = \"learning_rate\"\n",
        "LENGTH_COL = \"length_col\"\n",
        "LOG_FILE_NAME = \"log_file_name\"\n",
        "LOSS_FUNCTION = \"loss_function\"\n",
        "MAX_VALID_LOSS = \"max_valid_loss\"\n",
        "MIN_FRAME_ITEMS = \"min_frame_items\"\n",
        "MIN_VALID_EPOCHS = \"min_valid_epochs\"\n",
        "NORM_EMBED_VECS = \"norm_embed_vecs\"\n",
        "NUM_LENGTH_FRAMES = \"num_length_frames\"\n",
        "OPTIMIZER_NAME = \"optimizer_func\"\n",
        "PAD_LABEL = \"pad_label\"\n",
        "RANDOM_SAMPLING = \"random_sampling\"\n",
        "SEQUENTIAL_SAMPLING = \"sequential_sampling\"\n",
        "SGD_OPT = 'sgd'\n",
        "TEST_SET = \"test_set\"\n",
        "TRAIN_SET = \"train_set\"\n",
        "SAMPLING_TYPE = \"sampling_type\"\n",
        "SEED_VALUE = \"seed_value\"\n",
        "TEXT_COL = \"text_col\"\n",
        "TOKENS_COL = \"tokens_col\"\n",
        "UNK_LABEL = \"unk_label\"\n",
        "VALIDATION_SET = \"validation_set\""
      ],
      "metadata": {
        "id": "tGUQyi0vHPRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "with open(\"workshop/nb_config_lstm.json\", 'r', encoding='utf-8') as f:\n",
        "    config = json.load(f)\n",
        "print(config)\n",
        "\n",
        "added_words = [PAD_LABEL, UNK_LABEL]\n",
        "class Embedded_Words:\n",
        "    def __init__(self, model_file: str, added_pads: list, norm: bool) -> None:\n",
        "        self.vectors, self.w2i, self.i2w = self.read_model(model_file, added_pads, norm)\n",
        "\n",
        "    def read_model(self, model_file: str, added_pads: list, norm: bool) -> tuple:\n",
        "        with open(model_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [x.strip() for x in f.readlines()]\n",
        "\n",
        "        print(model_file)\n",
        "        print(len(lines))\n",
        "        print(lines[0])\n",
        "\n",
        "        num_word, dim = [int(x) for x in lines[0].split()]\n",
        "        vectors = np.zeros((num_word + len(added_pads), dim))\n",
        "        w2i = {}\n",
        "        i2w = {}\n",
        "        for line in tqdm(lines[1:]):\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            word_index = len(w2i)\n",
        "            v = np.array([float(x) for x in tokens[1:]])\n",
        "            if norm:\n",
        "                v = v / np.linalg.norm(v)\n",
        "            vectors[word_index] = v\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "\n",
        "        for word in added_pads:\n",
        "            word_index = len(w2i)\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "        \n",
        "        return vectors, w2i, i2w\n",
        "\n",
        "model_file = \"drive/MyDrive/ColabData/embed.model\"\n",
        "embedded_words = Embedded_Words(model_file, added_words, True)\n",
        "print(\"\")\n",
        "print(embedded_words.vectors.shape)\n",
        "print(embedded_words.vectors[embedded_words.vectors.shape[0]-5:,:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl6gHmdOIASU",
        "outputId": "2186c02b-c054-4377-92a4-63aabbf56b3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'adam_eps': 1e-08, 'batch_first': 1, 'batch_size': 64000, 'beta_one': 0, 'beta_two': 0.98, 'bideractional': 1, 'dropout': 0.5, 'drop_word': 0.1, 'early_stop_max_no_imp': 3, 'embed_words_file': 'workshop/embed.model', 'freeze_embeding': 1, 'hidden_dim': 512, 'learning_rate': 0.001, 'loss_function': 'cross_entropy_loss', 'log_file_name': 'workshop/lstm_nb_init.txt', 'min_valid_epochs': 10, 'max_train_length': 512, 'max_valid_loss': 0.145, 'mini_batch_size': 3000, 'min_frame_items': 500, 'norm_embed_vecs': 1, 'num_epochs': 100, 'num_layers': 1, 'num_length_frames': 12, 'optimizer_func': 'adam', 'rho': 0.95, 'seed_value': -1, 'sampling_type': 'sequential_sampling', 'label_col': 'sentiment', 'length_col': 'lengths', 'text_col': 'review', 'tokens_col': 'tokens', 'train_set': 'workshop/imdb_train_set.csv', 'test_set': 'workshop/imdb_test.csv', 'validation_set': 'workshop/imdb_val_set.csv'}\n",
            "drive/MyDrive/ColabData/embed.model\n",
            "85899\n",
            "85898 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85898/85898 [00:08<00:00, 10363.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(85900, 256)\n",
            "[[ 0.06298662 -0.03220133 -0.02866318  0.12042718  0.18225189 -0.06114065\n",
            "   0.05848268 -0.04147663  0.04435284 -0.00816747]\n",
            " [ 0.05723497 -0.02765993  0.0106675   0.13086651  0.09357034 -0.08705442\n",
            "  -0.00878455 -0.0667807  -0.00667503 -0.00585762]\n",
            " [-0.05147354 -0.00427562  0.09155177  0.13254647  0.11083994 -0.00598478\n",
            "   0.02139013 -0.03059529 -0.08231317  0.02991129]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_active_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "active_device = get_active_device()\n",
        "print(active_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkZhc7ghIXTi",
        "outputId": "d43bf830-fa7c-4c8a-aadc-13855abdc286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "\n",
        "BATCH_FIRST = True\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 config: dict,\n",
        "                 num_classes: int):\n",
        "        # Constructor.\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        print('Freeze embedding matrix = ' + str(config[FREEZE_EMBEDDING]))\n",
        "        print('load embedding model')\n",
        "        added_words = [PAD_LABEL, UNK_LABEL]\n",
        "        self.w2v_model = Embedded_Words(model_file, added_words, config[NORM_EMBED_VECS])\n",
        "        print('\\tw2v after padding: ' + str(self.w2v_model.vectors.shape))\n",
        "\n",
        "        print('generate embedding tensor')\n",
        "        # Set the embedding module.\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(self.w2v_model.vectors).to(active_device),\n",
        "            padding_idx = self.w2v_model.w2i[PAD_LABEL],\n",
        "            freeze=True) #config[FREEZE_EMBEDDING])\n",
        "        \n",
        "        # LSTM layer.\n",
        "        hidden_dim = config[HIDDEN_DIM]\n",
        "        bidirectional = bool(config[BIDIRECTIONAL])\n",
        "        self.lstm = nn.LSTM(self.w2v_model.vectors.shape[1],\n",
        "                            hidden_dim,\n",
        "                            num_layers=config[NUM_LAYERS],\n",
        "                            bidirectional=bidirectional,\n",
        "                            dropout=0,\n",
        "                            batch_first=BATCH_FIRST).to(active_device)\n",
        "        \n",
        "        # Set the dropout for the embedding layer and the lstm's output layer.\n",
        "        self.dropout = nn.Dropout(config[DROPOUT])\n",
        "\n",
        "        # Set the last layer, fully connected.\n",
        "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), num_classes).to(active_device)\n",
        "        \n",
        "    # The forward function.\n",
        "    def forward(self, texts, lengths):\n",
        "        # Get the embedding of the given text.\n",
        "        # text = [#batch size, sentence length]\n",
        "        #lengths = [#batch size]\n",
        "        embed_text = self.dropout(self.embedding(texts))\n",
        "        # embed_text = [#batch size, sentence length, embed dim]\n",
        "        \n",
        "        # Get the packed sentences.\n",
        "        packed_text = pack_padded_sequence(embed_text, lengths, batch_first=BATCH_FIRST)\n",
        "        # packed_text = [[sum lengths, embed dim], [#sequence length (#active batch items for ech length)]]\n",
        "\n",
        "        # Call the LSTM layer.\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_text)\n",
        "        # packed output = [[sum lengths, hidden dim], [#sequence length]]\n",
        "        # hidden = [1 (2 if bidirectional), #batch size, hidden dim]\n",
        "        # cell   = [1 (2 if bidirectional), #batch size, hidden dim]\n",
        "        \n",
        "        # unpack the output.\n",
        "        pad_packed_output, _ = pad_packed_sequence(packed_output, batch_first=BATCH_FIRST)\n",
        "        # pad_packed_output = [batch size, sentence length, hidden dim * 2]\n",
        "        \n",
        "        # Prmute the output before pooling.\n",
        "        permuted_output = self.dropout(pad_packed_output.permute(0, 2, 1))\n",
        "        # permuted_output = [batch size, hidden dim * 2, sentence length]\n",
        "\n",
        "        # Max pooling layer.        \n",
        "        pooled_output = F.max_pool1d(permuted_output, kernel_size=permuted_output.shape[2])\n",
        "        # pooled_output = [batch size, hidden dim * 2, 1]\n",
        "        \n",
        "        # Call the linear full connected layer, after droping out.\n",
        "        logits = self.fc(torch.squeeze(pooled_output, dim=2))\n",
        "        # logits = [batch size, #classes]\n",
        "        \n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "iJNzxtpQIbvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "import pandas as pd\n",
        "import copy\n",
        "import random\n",
        "\n",
        "def break_by_batch_size(df: pd.DataFrame, config: dict) -> list:\n",
        "    sorted_df = df.sort_values(by=config[LENGTH_COL], axis=0, ascending=False, ignore_index=True)\n",
        "    tokens = sorted_df[config[TOKENS_COL]].to_list()\n",
        "    labels = sorted_df[config[LABEL_COL]].to_list()\n",
        "    lengths = sorted_df[config[LENGTH_COL]].to_list()\n",
        "    \n",
        "    df_list = []\n",
        "    batch_size = config[MINI_BATCH_SIZE]\n",
        "    max_valid_length = config[MAX_TRAIN_LENGTH]\n",
        "    header = {config[TOKENS_COL]:[], config[LABEL_COL]:[], config[LENGTH_COL]:[]}\n",
        "    row_index = 0\n",
        "    num_rows = len(labels)\n",
        "    while row_index < num_rows:\n",
        "        num_words = 0\n",
        "        curr_df = copy.deepcopy(header)\n",
        "        while num_words < batch_size and row_index < num_rows:\n",
        "            actual_length = min(max_valid_length, lengths[row_index])\n",
        "            num_words += actual_length\n",
        "            curr_df[config[TOKENS_COL]].append(tokens[row_index][:max_valid_length])\n",
        "            curr_df[config[LABEL_COL]].append(labels[row_index])\n",
        "            curr_df[config[LENGTH_COL]].append(actual_length)\n",
        "            row_index += 1\n",
        "        \n",
        "        df_list.append(pd.DataFrame(curr_df))\n",
        "        \n",
        "    return df_list\n",
        "\n",
        "DROP_WORD_PROB = -1\n",
        "def df_to_dataloader(df: pd.DataFrame, w2v_model: Embedded_Words, config: dict, sampling_type: str) -> DataLoader:\n",
        "    sorted_df = df.sort_values(by=config[LENGTH_COL], axis=0, ascending=False, ignore_index=True)\n",
        "    texts = sorted_df[config[TOKENS_COL]].values.tolist()\n",
        "    labels = sorted_df[config[LABEL_COL]].values.tolist()\n",
        "    lengths = sorted_df[config[LENGTH_COL]].to_list()\n",
        "    max_len = sorted_df[config[TOKENS_COL]].map(len).max()\n",
        "\n",
        "    indexed_texts = []\n",
        "    for sentence in texts:\n",
        "        sentence += [PAD_LABEL] * (max_len - len(sentence))\n",
        "        ids = []\n",
        "        for word in sentence:\n",
        "            if word not in w2v_model.w2i:\n",
        "                ids.append(w2v_model.w2i[UNK_LABEL])\n",
        "            elif np.random.random() < DROP_WORD_PROB:\n",
        "                ids.append(w2v_model.w2i[UNK_LABEL])\n",
        "            else:\n",
        "                ids.append(w2v_model.w2i[word])\n",
        "        \n",
        "        indexed_texts.append(ids)\n",
        "        \n",
        "    inputs, labels, lengths = tuple(torch.tensor(data) for data in [indexed_texts, labels, lengths])\n",
        "\n",
        "    data = TensorDataset(inputs, labels, lengths)\n",
        "    \n",
        "    if sampling_type == RANDOM_SAMPLING:\n",
        "        sampler = RandomSampler(data)\n",
        "    elif sampling_type == SEQUENTIAL_SAMPLING:\n",
        "        sampler = SequentialSampler(data)\n",
        "    else:\n",
        "        print('Wrong Sampling Type: ' + sampling_type)\n",
        "        return None\n",
        "        \n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=config[BATCH_SIZE])\n",
        "    return dataloader\n",
        "    \n",
        "def get_data_loaders(input_df: pd.DataFrame,\n",
        "                     w2v_model: Embedded_Words,\n",
        "                     config: dict, \n",
        "                     sampling_type: str,\n",
        "                     break_df_func) -> list:\n",
        "    input_df[config[TOKENS_COL]] = input_df[config[TEXT_COL]].apply(lambda x: x.split(' '))\n",
        "    input_df[config[LENGTH_COL]] = input_df[config[TOKENS_COL]].map(len)\n",
        "    df_list = break_df_func(input_df, config)\n",
        "    dataloaders = []\n",
        "    for df in df_list:\n",
        "        dataloader = df_to_dataloader(df, w2v_model, config, sampling_type)\n",
        "        dataloaders.append(dataloader)\n",
        "        \n",
        "    return dataloaders\n",
        "\n",
        "RHO = 0.95\n",
        "LEARNING_RATE = 0.001\n",
        "OPT_NAME = ADAM_OPT\n",
        "BETA_ONE = 0\n",
        "BETA_TWO = 0.98\n",
        "ADAM_EPS = 0.00000001\n",
        "def get_optimizer(parameters):\n",
        "    optimizer = None\n",
        "    if OPT_NAME == ADADELATA_OPT:\n",
        "        optimizer = optim.Adadelta(parameters,\n",
        "                                   lr=LEARNING_RATE,\n",
        "                                   rho=RHO)\n",
        "    elif OPT_NAME == SGD_OPT:\n",
        "        optimizer = optim.SGD(parameters, LEARNING_RATE)\n",
        "    elif OPT_NAME == ADAM_OPT:\n",
        "        optimizer = optim.Adam(parameters,\n",
        "                               lr=LEARNING_RATE,\n",
        "                               betas=(BETA_ONE,BETA_TWO,),\n",
        "                               eps=ADAM_EPS)\n",
        "    else:\n",
        "        print('Wrong optimizer name: ' + OPT_NAME)\n",
        "        \n",
        "    return optimizer\n",
        "    \n",
        "    \n",
        "def get_loss_function(func_name: str):\n",
        "    loss_func = None\n",
        "    if func_name == CROSS_ENTROP_LOSS:\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "    elif func_name == BCE_LOSS:\n",
        "        loss_func = nn.BCELoss()\n",
        "    else:\n",
        "        print('Wrong loss function name: ' + func_name)\n",
        "        \n",
        "    return loss_func\n",
        "\n",
        "\n",
        "def set_seed(seed_value: int):\n",
        "    if seed_value >= 0:\n",
        "        random.seed(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        torch.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n"
      ],
      "metadata": {
        "id": "Rresr1f6JIO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def test(model: nn.Module, dataloaders: list):\n",
        "    corrects = 0\n",
        "    evaluated = 0\n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    for dl in dataloaders:\n",
        "        for texts, labels, lengths in dl:\n",
        "            texts = texts.to(active_device)\n",
        "            labels = labels.to(active_device)\n",
        "            lengths = lengths.to(torch.device(\"cpu\"))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(texts, lengths)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            corrects += (preds == labels).sum().item()\n",
        "            evaluated += texts.shape[0]\n",
        "        \n",
        "    accuracy = corrects / evaluated\n",
        "    run_time = time.time() - start_time\n",
        "    return accuracy, run_time\n"
      ],
      "metadata": {
        "id": "2j1YBlXcKV_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def end_train(last_model: nn.Module, opt_model: nn.Module, test_dl: list, val_dl: list, config: dict, log_file):\n",
        "    accuracy, run_time = test(last_model, test_dl)\n",
        "    str_acc = \"{:.5f}\".format(accuracy)\n",
        "    str_time = \"{:.1f}\".format(run_time)\n",
        "    log_file.write('Last Model\\t' + str_acc + '\\t' + str_time + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Last Model\\t' + str_acc + '\\t' + str_time)\n",
        "        \n",
        "    # Print optimal\n",
        "    opt_acc, run_time = test(opt_model, test_dl)\n",
        "    val_acc, run_time = test(opt_model, val_dl)\n",
        "\n",
        "    test_acc = \"test: {:.5f}\".format(opt_acc)\n",
        "    val_acc = \"val: {:.5f}\".format(val_acc)\n",
        "    log_file.write('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc)\n",
        "    log_file.write(\"\\nConfiguraton:\\n\")\n",
        "    sorted_params = sorted(config.items())\n",
        "    for param in sorted_params:\n",
        "        log_file.write(param[0] + \"\\t\" + str(param[1]) + \"\\n\")\n",
        "        log_file.flush()\n"
      ],
      "metadata": {
        "id": "lTmmREDSK2E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def set_seed(seed_value: int):\n",
        "    if seed_value >= 0:\n",
        "        random.seed(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        torch.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "import time\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import pandas as pd\n",
        "NUM_EPOCHS = 40\n",
        "MAX_NO_IMP = 3\n",
        "MAX_VALID_LOSS = 0.145\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self, config: dict) -> tuple:\n",
        "        print('latm trainer - start')\n",
        "        log_file = open(config[LOG_FILE_NAME], \"w\", encoding=\"utf-8\")\n",
        "        seed_value = config[SEED_VALUE]\n",
        "        set_seed(seed_value)\n",
        "            \n",
        "        train_df = pd.read_csv(config[TRAIN_SET])\n",
        "        val_df = pd.read_csv(config[VALIDATION_SET])\n",
        "        test_df = pd.read_csv(config[TEST_SET])\n",
        "\n",
        "        pending_model = LSTM(config=config, num_classes=train_df[config[LABEL_COL]].unique().shape[0])\n",
        "        optimal_model = None\n",
        "        \n",
        "        optimizer =  get_optimizer(pending_model.parameters())\n",
        "        loss_func = nn.NLLLoss()\n",
        "        log_soft_max = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        num_epochs = NUM_EPOCHS\n",
        "        print('start training loops. #epochs = ' + str(num_epochs))\n",
        "        print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^11} | {'Test Acc':^9} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*50)  \n",
        "        \n",
        "        log_file.write(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^11} | {'Test Acc':^9} | {'Val Acc':^9} | {'Elapsed':^9}\\n\")\n",
        "        log_file.write(\"-\"*50 + \"\\n\")\n",
        "            \n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_test_acc = 0\n",
        "        best_test_epoch = -1\n",
        "        best_epoch = -1\n",
        "        min_loss = 100\n",
        "        num_no_imp = 0\n",
        "        validation_dl = get_data_loaders(val_df, pending_model.w2v_model, config=config, sampling_type=SEQUENTIAL_SAMPLING, break_df_func=break_by_batch_size)\n",
        "        test_dl = get_data_loaders(test_df, pending_model.w2v_model, config=config, sampling_type=SEQUENTIAL_SAMPLING, break_df_func=break_by_batch_size)\n",
        "        for i in range(num_epochs):\n",
        "            epoch = i + 1\n",
        "            epoch_start_time = time.time()\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            train_dl = get_data_loaders(train_df, pending_model.w2v_model, config=config, sampling_type=SEQUENTIAL_SAMPLING, break_df_func=break_by_batch_size)\n",
        "            random.shuffle(train_dl)\n",
        "            pending_model.train()\n",
        "            for dl in train_dl:\n",
        "                for texts, labels, lengths in dl:\n",
        "                    texts = texts.to(active_device)\n",
        "                    labels = labels.to(active_device)\n",
        "                    lengths = lengths.to(torch.device(\"cpu\"))\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = pending_model(texts, lengths)\n",
        "                    weights = log_soft_max(logits)\n",
        "                    loss = loss_func(weights, labels)\n",
        "                    total_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "            avg_loss = total_loss / num_batches\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            \n",
        "            # Validation test.\n",
        "            val_acc, val_time = test(pending_model, validation_dl)\n",
        "            train_acc, train_time = test(pending_model, train_dl)\n",
        "            test_acc, test_time = test(pending_model, test_dl)\n",
        "            val_acc *= 100\n",
        "            train_acc *= 100\n",
        "            test_acc *= 100\n",
        "            print(f\"{epoch:^7} | {avg_loss:^12.6f}  {train_acc:^9.2f} |  {test_acc:^9.2f} |  {val_acc:^9.4f} | {epoch_time:^9.2f}\")\n",
        "            log_file.write(f\"{epoch:^7} | {avg_loss:^12.6f}  {train_acc:^9.2f} |  {test_acc:^9.2f} |  {val_acc:^9.4f} | {epoch_time:^9.2f}\\n\")\n",
        "            log_file.flush()\n",
        "                \n",
        "            if avg_loss < min_loss:\n",
        "                min_loss = avg_loss\n",
        "                num_no_imp = 0\n",
        "            else:\n",
        "                num_no_imp += 1\n",
        "                \n",
        "            if num_no_imp > MAX_NO_IMP:\n",
        "                print('early stop exit')\n",
        "                log_file.write('\\tEarly Stop exit\\n')\n",
        "                log_file.flush()\n",
        "                break\n",
        "            \n",
        "            if epoch < config[MIN_VALID_EPOCHS]:\n",
        "                continue\n",
        "            \n",
        "            if avg_loss > MAX_VALID_LOSS:\n",
        "                continue\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                optimal_model = copy.deepcopy(pending_model)\n",
        "                best_epoch = epoch\n",
        "\n",
        "            if test_acc > best_test_acc:\n",
        "              best_test_acc = test_acc\n",
        "              best_test_epoch = epoch\n",
        "        \n",
        "        print('train_lstm_nlp - end')\n",
        "        print(\"best val: acc={:.3f}\".format(best_val_acc) + \", epoch=\" + str(best_epoch))\n",
        "        print(\"best test: acc={:.3f}\".format(best_test_acc) + \", epoch=\" + str(best_test_epoch))\n",
        "        end_train(pending_model, optimal_model, test_dl, validation_dl, config, log_file)\n",
        "        return pending_model, optimal_model, best_epoch\n"
      ],
      "metadata": {
        "id": "fR0Gry13LJFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"workshop/nb_config_lstm.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  actual_config = json.load(f)\n",
        "\n",
        "trainer = Trainer()\n",
        "last_model, opt_model, best_epoch = trainer.train(actual_config)\n",
        "print(best_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib3bZjwsPTsl",
        "outputId": "64c165cf-596a-4c26-9e2a-a12b934ca853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "latm trainer - start\n",
            "Freeze embedding matrix = 1\n",
            "load embedding model\n",
            "drive/MyDrive/ColabData/embed.model\n",
            "85899\n",
            "85898 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85898/85898 [00:08<00:00, 9912.34it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tw2v after padding: (85900, 256)\n",
            "generate embedding tensor\n",
            "start training loops. #epochs = 40\n",
            " Epoch  |  Train Loss  |  Train Acc  | Test Acc  |  Val Acc  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   0.446636      86.76   |    86.43   |   85.5792  |  112.06  \n",
            "   2    |   0.320785      89.55   |    89.21   |   87.6281  |  111.15  \n",
            "   3    |   0.290634      90.90   |    90.09   |   88.5737  |  111.65  \n",
            "   4    |   0.265796      91.77   |    91.01   |   88.8889  |  112.01  \n",
            "   5    |   0.247293      92.46   |    91.27   |   89.3617  |  110.36  \n",
            "   6    |   0.237611      93.01   |    91.40   |   90.1497  |  111.86  \n",
            "   7    |   0.226507      93.70   |    91.86   |   90.6225  |  111.21  \n",
            "   8    |   0.221044      92.30   |    90.38   |   89.4405  |  111.94  \n",
            "   9    |   0.257943      93.77   |    91.68   |   90.7801  |  110.35  \n",
            "  10    |   0.200914      94.51   |    92.15   |   91.4106  |  112.23  \n",
            "  11    |   0.188575      94.74   |    91.69   |   90.1497  |  110.77  \n",
            "  12    |   0.218146      92.92   |    90.17   |   88.9677  |  111.73  \n",
            "  13    |   0.247012      94.96   |    91.81   |   90.7013  |  110.54  \n",
            "  14    |   0.188704      95.60   |    92.14   |   91.4894  |  111.72  \n",
            "  15    |   0.172011      96.14   |    92.25   |   91.4106  |  111.10  \n",
            "  16    |   0.160174      96.60   |    92.39   |   91.2530  |  111.59  \n",
            "  17    |   0.157984      96.32   |    92.00   |   91.1742  |  110.32  \n",
            "  18    |   0.146089      96.21   |    91.65   |   91.0954  |  111.95  \n",
            "  19    |   0.136171      96.36   |    91.58   |   91.2530  |  111.15  \n",
            "  20    |   0.134630      97.35   |    92.20   |   91.5682  |  112.05  \n",
            "  21    |   0.133856      97.35   |    92.21   |   90.5437  |  110.48  \n",
            "  22    |   0.122368      97.70   |    92.40   |   91.8834  |  112.61  \n",
            "  23    |   0.118998      97.71   |    92.34   |   91.5682  |  110.45  \n",
            "  24    |   0.106874      98.05   |    92.46   |   91.3318  |  111.96  \n",
            "  25    |   0.105610      98.20   |    92.24   |   91.0954  |  110.48  \n",
            "  26    |   0.107336      98.26   |    92.25   |   91.0165  |  112.55  \n",
            "  27    |   0.097176      98.44   |    92.40   |   91.7258  |  110.48  \n",
            "  28    |   0.096203      98.53   |    92.44   |   92.1986  |  111.74  \n",
            "  29    |   0.085324      98.51   |    92.14   |   91.0165  |  110.41  \n",
            "  30    |   0.088048      97.45   |    90.70   |   91.2530  |  112.59  \n",
            "  31    |   0.081235      98.76   |    92.25   |   91.7258  |  110.56  \n",
            "  32    |   0.077769      99.05   |    92.45   |   91.4106  |  112.05  \n",
            "  33    |   0.079002      99.20   |    92.54   |   91.8046  |  110.67  \n",
            "  34    |   0.075548      99.18   |    92.28   |   91.8046  |  112.31  \n",
            "  35    |   0.076335      99.00   |    92.16   |   91.1742  |  110.60  \n",
            "  36    |   0.070038      98.86   |    91.62   |   91.3318  |  111.74  \n",
            "  37    |   0.064950      99.30   |    92.39   |   91.9622  |  110.29  \n",
            "  38    |   0.062988      99.44   |    92.44   |   91.7258  |  112.70  \n",
            "  39    |   0.064018      99.51   |    92.65   |   91.2530  |  110.75  \n",
            "  40    |   0.059933      99.62   |    92.31   |   91.3318  |  111.95  \n",
            "train_lstm_nlp - end\n",
            "best val: acc=92.199, epoch=28\n",
            "best test: acc=92.652, epoch=39\n",
            "Last Model\t0.92308\t46.8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:777: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:968.)\n",
            "  result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Model\tTest=test: 0.92444\tVal=val: 0.92199\n",
            "28\n"
          ]
        }
      ]
    }
  ]
}