{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr9QRHdyQ5vY",
        "outputId": "6900976f-d321-4c68-b10d-597092bbd8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "UNK_LABEL = \"unk_label\"\n",
        "PAD_LABEL = \"pad_label\"\n",
        "added_words = [PAD_LABEL, UNK_LABEL]\n",
        "class Embedded_Words:\n",
        "    def __init__(self, model_file: str, added_pads: list, norm: bool) -> None:\n",
        "        self.vectors, self.w2i, self.i2w = self.read_model(model_file, added_pads, norm)\n",
        "\n",
        "    def read_model(self, model_file: str, added_pads: list, norm: bool) -> tuple:\n",
        "        with open(model_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [x.strip() for x in f.readlines()]\n",
        "\n",
        "        print(model_file)\n",
        "        print(len(lines))\n",
        "        print(lines[0])\n",
        "\n",
        "        num_word, dim = [int(x) for x in lines[0].split()]\n",
        "        vectors = np.zeros((num_word + len(added_pads), dim))\n",
        "        w2i = {}\n",
        "        i2w = {}\n",
        "        for line in tqdm(lines[1:]):\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            word_index = len(w2i)\n",
        "            v = np.array([float(x) for x in tokens[1:]])\n",
        "            if norm:\n",
        "                v = v / np.linalg.norm(v)\n",
        "            vectors[word_index] = v\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "\n",
        "        for word in added_pads:\n",
        "            word_index = len(w2i)\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "        \n",
        "        return vectors, w2i, i2w\n",
        "\n",
        "model_file = \"drive/MyDrive/ColabData/embed.model\"\n",
        "embedded_words = Embedded_Words(model_file, added_words, True)\n",
        "print(\"\")\n",
        "print(embedded_words.vectors.shape)\n",
        "print(embedded_words.vectors[embedded_words.vectors.shape[0]-5:,:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVGB4Tj0Td4l",
        "outputId": "17d0a77f-f933-46bb-ceda-8cb5313abc11"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/ColabData/embed.model\n",
            "85899\n",
            "85898 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85898/85898 [00:17<00:00, 4834.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(85900, 256)\n",
            "[[ 0.06298662 -0.03220133 -0.02866318  0.12042718  0.18225189 -0.06114065\n",
            "   0.05848268 -0.04147663  0.04435284 -0.00816747]\n",
            " [ 0.05723497 -0.02765993  0.0106675   0.13086651  0.09357034 -0.08705442\n",
            "  -0.00878455 -0.0667807  -0.00667503 -0.00585762]\n",
            " [-0.05147354 -0.00427562  0.09155177  0.13254647  0.11083994 -0.00598478\n",
            "   0.02139013 -0.03059529 -0.08231317  0.02991129]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_active_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "active_device = get_active_device()\n",
        "print(active_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7j_-PDZUVAy",
        "outputId": "598d68bc-2748-476b-c95a-db85e815fb88"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "\n",
        "BATCH_FIRST = True\n",
        "FREEZE_EMBEDDING = True\n",
        "NORM_EMBED_VECS = True\n",
        "HIDDEN_DIM = 512\n",
        "BIDIRECTIONAL = True\n",
        "NUM_LAYERS = 1\n",
        "DROPOUT = 0.5\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes: int):\n",
        "        # Constructor.\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        print('Freeze embedding matrix = ' + str(FREEZE_EMBEDDING))\n",
        "        print('load embedding model')\n",
        "        added_words = [PAD_LABEL, UNK_LABEL]\n",
        "        self.w2v_model = Embedded_Words(model_file, added_words, NORM_EMBED_VECS)\n",
        "        print('\\tw2v after padding: ' + str(self.w2v_model.vectors.shape))\n",
        "\n",
        "        print('generate embedding tensor')\n",
        "        # Set the embedding module.\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(self.w2v_model.vectors).to(active_device),\n",
        "            padding_idx = self.w2v_model.w2i[PAD_LABEL],\n",
        "            freeze=True) #config[FREEZE_EMBEDDING])\n",
        "        \n",
        "        # LSTM layer.\n",
        "        hidden_dim = HIDDEN_DIM\n",
        "        self.lstm = nn.LSTM(self.w2v_model.vectors.shape[1],\n",
        "                            hidden_dim,\n",
        "                            num_layers=NUM_LAYERS,\n",
        "                            bidirectional=BIDIRECTIONAL,\n",
        "                            dropout=0,\n",
        "                            batch_first=BATCH_FIRST).to(active_device)\n",
        "        \n",
        "        # Set the dropout for the embedding layer and the lstm's output layer.\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "        # Set the last layer, fully connected.\n",
        "        self.fc = nn.Linear(hidden_dim * (2 if BIDIRECTIONAL else 1), num_classes).to(active_device)\n",
        "\n",
        "    def embed_text(self, texts):\n",
        "      return self.dropout(self.embedding(texts))\n",
        "        \n",
        "    # The forward function.\n",
        "    def forward(self, texts, lengths):\n",
        "        # Get the embedding of the given text.\n",
        "        # texts = [#batch size, sentence length, embed dim]\n",
        "        #lengths = [#batch size]\n",
        "        #embed_text = self.dropout(self.embedding(texts))\n",
        "        # embed_text = [#batch size, sentence length, embed dim]\n",
        "        \n",
        "        # Get the packed sentences.\n",
        "        packed_text = pack_padded_sequence(texts, lengths, batch_first=BATCH_FIRST)\n",
        "        # packed_text = [[sum lengths, embed dim], [#sequence length (#active batch items for ech length)]]\n",
        "\n",
        "        # Call the LSTM layer.\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_text)\n",
        "        # packed output = [[sum lengths, hidden dim], [#sequence length]]\n",
        "        # hidden = [1 (2 if bidirectional), #batch size, hidden dim]\n",
        "        # cell   = [1 (2 if bidirectional), #batch size, hidden dim]\n",
        "        \n",
        "        # unpack the output.\n",
        "        pad_packed_output, _ = pad_packed_sequence(packed_output, batch_first=BATCH_FIRST)\n",
        "        # pad_packed_output = [batch size, sentence length, hidden dim * 2]\n",
        "        \n",
        "        # Prmute the output before pooling.\n",
        "        permuted_output = self.dropout(pad_packed_output.permute(0, 2, 1))\n",
        "        # permuted_output = [batch size, hidden dim * 2, sentence length]\n",
        "\n",
        "        # Max pooling layer.        \n",
        "        pooled_output = F.max_pool1d(permuted_output, kernel_size=permuted_output.shape[2])\n",
        "        # pooled_output = [batch size, hidden dim * 2, 1]\n",
        "        \n",
        "        # Call the linear full connected layer, after droping out.\n",
        "        logits = self.fc(torch.squeeze(pooled_output, dim=2))\n",
        "        # logits = [batch size, #classes]\n",
        "        \n",
        "        return logits"
      ],
      "metadata": {
        "id": "UUU17J5dUenl"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "import pandas as pd\n",
        "import copy\n",
        "import random\n",
        "\n",
        "LENGTH_COL = \"lengths\"\n",
        "TOKENS_COL = \"tokens\"\n",
        "LABEL_COL = \"sentiment\"\n",
        "MINI_BATCH_SIZE = 3000\n",
        "MAX_TRAIN_LENGTH = 512\n",
        "def break_by_batch_size(df: pd.DataFrame) -> list:\n",
        "    sorted_df = df.sort_values(by=LENGTH_COL, axis=0, ascending=False, ignore_index=True)\n",
        "    tokens = sorted_df[TOKENS_COL].to_list()\n",
        "    labels = sorted_df[LABEL_COL].to_list()\n",
        "    lengths = sorted_df[LENGTH_COL].to_list()\n",
        "    \n",
        "    df_list = []\n",
        "    header = {TOKENS_COL:[], LABEL_COL:[], LENGTH_COL:[]}\n",
        "    row_index = 0\n",
        "    num_rows = len(labels)\n",
        "    while row_index < num_rows:\n",
        "        num_words = 0\n",
        "        curr_df = copy.deepcopy(header)\n",
        "        while num_words < MINI_BATCH_SIZE and row_index < num_rows:\n",
        "            actual_length = min(MAX_TRAIN_LENGTH, lengths[row_index])\n",
        "            num_words += actual_length\n",
        "            curr_df[TOKENS_COL].append(tokens[row_index][:MAX_TRAIN_LENGTH])\n",
        "            curr_df[LABEL_COL].append(labels[row_index])\n",
        "            curr_df[LENGTH_COL].append(actual_length)\n",
        "            row_index += 1\n",
        "        \n",
        "        df_list.append(pd.DataFrame(curr_df))\n",
        "        \n",
        "    return df_list\n",
        "\n",
        "DROP_WORD_PROB = -1\n",
        "RANDOM_SAMPLING = \"random_sampling\"\n",
        "SEQUENTIAL_SAMPLING = \"sequential_sampling\"\n",
        "def df_to_dataloader(df: pd.DataFrame, w2v_model: Embedded_Words, sampling_type: str) -> DataLoader:\n",
        "    sorted_df = df.sort_values(by=LENGTH_COL, axis=0, ascending=False, ignore_index=True)\n",
        "    texts = sorted_df[TOKENS_COL].values.tolist()\n",
        "    labels = sorted_df[LABEL_COL].values.tolist()\n",
        "    lengths = sorted_df[LENGTH_COL].to_list()\n",
        "    max_len = sorted_df[TOKENS_COL].map(len).max()\n",
        "\n",
        "    indexed_texts = []\n",
        "    for sentence in texts:\n",
        "        sentence += [PAD_LABEL] * (max_len - len(sentence))\n",
        "        ids = []\n",
        "        for word in sentence:\n",
        "            if word not in w2v_model.w2i:\n",
        "                ids.append(w2v_model.w2i[UNK_LABEL])\n",
        "            elif np.random.random() < DROP_WORD_PROB:\n",
        "                ids.append(w2v_model.w2i[UNK_LABEL])\n",
        "            else:\n",
        "                ids.append(w2v_model.w2i[word])\n",
        "        \n",
        "        indexed_texts.append(ids)\n",
        "        \n",
        "    inputs, labels, lengths = tuple(torch.tensor(data) for data in [indexed_texts, labels, lengths])\n",
        "\n",
        "    data = TensorDataset(inputs, labels, lengths)\n",
        "    \n",
        "    if sampling_type == RANDOM_SAMPLING:\n",
        "        sampler = RandomSampler(data)\n",
        "    elif sampling_type == SEQUENTIAL_SAMPLING:\n",
        "        sampler = SequentialSampler(data)\n",
        "    else:\n",
        "        print('Wrong Sampling Type: ' + sampling_type)\n",
        "        return None\n",
        "        \n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=MINI_BATCH_SIZE)\n",
        "    return dataloader\n",
        "\n",
        "TEXT_COL = \"review\"\n",
        "def get_data_loaders(input_df: pd.DataFrame,\n",
        "                     w2v_model: Embedded_Words,\n",
        "                     sampling_type: str,\n",
        "                     break_df_func) -> list:\n",
        "    input_df[TOKENS_COL] = input_df[TEXT_COL].apply(lambda x: x.split(' '))\n",
        "    input_df[LENGTH_COL] = input_df[TOKENS_COL].map(len)\n",
        "    df_list = break_df_func(input_df)\n",
        "    dataloaders = []\n",
        "    for df in df_list:\n",
        "        dataloader = df_to_dataloader(df, w2v_model, sampling_type)\n",
        "        dataloaders.append(dataloader)\n",
        "        \n",
        "    return dataloaders\n",
        "\n",
        "RHO = 0.95\n",
        "LEARNING_RATE = 0.00075\n",
        "OPT_NAME = \"adam\"\n",
        "BETA_ONE = 0\n",
        "BETA_TWO = 0.98\n",
        "ADAM_EPS = 0.00000001\n",
        "ADADELATA_OPT = \"adadelta\"\n",
        "SGD_OPT = \"sgd\"\n",
        "ADAM_OPT = \"adam\"\n",
        "def get_optimizer(parameters):\n",
        "    optimizer = None\n",
        "    if OPT_NAME == ADADELATA_OPT:\n",
        "        optimizer = optim.Adadelta(parameters,\n",
        "                                   lr=LEARNING_RATE,\n",
        "                                   rho=RHO)\n",
        "    elif OPT_NAME == SGD_OPT:\n",
        "        optimizer = optim.SGD(parameters, LEARNING_RATE)\n",
        "    elif OPT_NAME == ADAM_OPT:\n",
        "        optimizer = optim.Adam(parameters,\n",
        "                               lr=LEARNING_RATE,\n",
        "                               betas=(BETA_ONE,BETA_TWO,),\n",
        "                               eps=ADAM_EPS)\n",
        "    else:\n",
        "        print('Wrong optimizer name: ' + OPT_NAME)\n",
        "        \n",
        "    return optimizer\n",
        "    \n",
        "\n",
        "CROSS_ENTROP_LOSS = \"cross_entropy_loss\"\n",
        "BCE_LOSS = \"bce_loss\"\n",
        "def get_loss_function(func_name: str):\n",
        "    loss_func = None\n",
        "    if func_name == CROSS_ENTROP_LOSS:\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "    elif func_name == BCE_LOSS:\n",
        "        loss_func = nn.BCELoss()\n",
        "    else:\n",
        "        print('Wrong loss function name: ' + func_name)\n",
        "        \n",
        "    return loss_func\n",
        "\n",
        "\n",
        "def set_seed(seed_value: int):\n",
        "    if seed_value >= 0:\n",
        "        random.seed(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        torch.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n"
      ],
      "metadata": {
        "id": "DhjHuVDkVrlh"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def test(model: nn.Module, dataloaders: list):\n",
        "    corrects = 0\n",
        "    evaluated = 0\n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    for dl in dataloaders:\n",
        "        for texts, labels, lengths in dl:\n",
        "            texts = texts.to(active_device)\n",
        "            labels = labels.to(active_device)\n",
        "            lengths = lengths.to(torch.device(\"cpu\"))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embed_text = model.embed_text(texts)\n",
        "                logits = model(embed_text, lengths)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            corrects += (preds == labels).sum().item()\n",
        "            evaluated += texts.shape[0]\n",
        "        \n",
        "    accuracy = corrects / evaluated\n",
        "    run_time = time.time() - start_time\n",
        "    return accuracy, run_time"
      ],
      "metadata": {
        "id": "MQUjZFXCYeQD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def end_train(last_model: nn.Module, opt_model: nn.Module, test_dl: list, val_dl: list, log_file):\n",
        "    accuracy, run_time = test(last_model, test_dl)\n",
        "    str_acc = \"{:.5f}\".format(accuracy)\n",
        "    str_time = \"{:.1f}\".format(run_time)\n",
        "    log_file.write('Last Model\\t' + str_acc + '\\t' + str_time + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Last Model\\t' + str_acc + '\\t' + str_time)\n",
        "        \n",
        "    # Print optimal\n",
        "    opt_acc, run_time = test(opt_model, test_dl)\n",
        "    val_acc, run_time = test(opt_model, val_dl)\n",
        "\n",
        "    test_acc = \"test: {:.5f}\".format(opt_acc)\n",
        "    val_acc = \"val: {:.5f}\".format(val_acc)\n",
        "    log_file.write('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc)\n",
        "    log_file.write(\"\\nConfiguraton:\\n\")\n"
      ],
      "metadata": {
        "id": "KfiGzpsCYk9m"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def optimize_linear(grad, eps, norm=np.inf):\n",
        "    \"\"\"\n",
        "    Solves for the optimal input to a linear function under a norm constraint.\n",
        "    Optimal_perturbation = argmax_{eta, ||eta||_{norm} < eps} dot(eta, grad)\n",
        "    :param grad: Tensor, shape (N, d_1, ...). Batch of gradients\n",
        "    :param eps: float. Scalar specifying size of constraint region\n",
        "    :param norm: np.inf, 1, or 2. Order of norm constraint.\n",
        "    :returns: Tensor, shape (N, d_1, ...). Optimal perturbation\n",
        "    \"\"\"\n",
        "\n",
        "    red_ind = list(range(1, len(grad.size())))\n",
        "    avoid_zero_div = torch.tensor(1e-12, dtype=grad.dtype, device=grad.device)\n",
        "    if norm == np.inf:\n",
        "        # Take sign of gradient\n",
        "        optimal_perturbation = torch.sign(grad)\n",
        "    elif norm == 1:\n",
        "        abs_grad = torch.abs(grad)\n",
        "        sign = torch.sign(grad)\n",
        "        red_ind = list(range(1, len(grad.size())))\n",
        "        abs_grad = torch.abs(grad)\n",
        "        ori_shape = [1] * len(grad.size())\n",
        "        ori_shape[0] = grad.size(0)\n",
        "\n",
        "        max_abs_grad, _ = torch.max(abs_grad.view(grad.size(0), -1), 1)\n",
        "        max_mask = abs_grad.eq(max_abs_grad.view(ori_shape)).to(torch.float)\n",
        "        num_ties = max_mask\n",
        "        for red_scalar in red_ind:\n",
        "            num_ties = torch.sum(num_ties, red_scalar, keepdim=True)\n",
        "        optimal_perturbation = sign * max_mask / num_ties\n",
        "        # TODO integrate below to a test file\n",
        "        # check that the optimal perturbations have been correctly computed\n",
        "        opt_pert_norm = optimal_perturbation.abs().sum(dim=red_ind)\n",
        "        assert torch.all(opt_pert_norm == torch.ones_like(opt_pert_norm))\n",
        "    elif norm == 2:\n",
        "        square = torch.max(avoid_zero_div, torch.sum(grad ** 2, red_ind, keepdim=True))\n",
        "        optimal_perturbation = grad / torch.sqrt(square)\n",
        "        # TODO integrate below to a test file\n",
        "        # check that the optimal perturbations have been correctly computed\n",
        "        opt_pert_norm = (\n",
        "            optimal_perturbation.pow(2).sum(dim=red_ind, keepdim=True).sqrt()\n",
        "        )\n",
        "        one_mask = (square <= avoid_zero_div).to(torch.float) * opt_pert_norm + (\n",
        "            square > avoid_zero_div\n",
        "        ).to(torch.float)\n",
        "        assert torch.allclose(opt_pert_norm, one_mask, rtol=1e-05, atol=1e-08)\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            \"Only L-inf, L1 and L2 norms are \" \"currently implemented.\"\n",
        "        )\n",
        "\n",
        "    # Scale perturbation to be the solution for the norm=eps rather than\n",
        "    # norm=1 problem\n",
        "    scaled_perturbation = eps * optimal_perturbation\n",
        "    return scaled_perturbation\n",
        "\n",
        "\n",
        "def fast_gradient_method(\n",
        "    model_fn,\n",
        "    x,\n",
        "    eps,\n",
        "    norm,\n",
        "    lengths,\n",
        "    clip_min=None,\n",
        "    clip_max=None,\n",
        "    y=None,\n",
        "    sanity_checks=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    PyTorch implementation of the Fast Gradient Method.\n",
        "    :param model_fn: a callable that takes an input tensor and returns the model logits.\n",
        "    :param x: input tensor.\n",
        "    :param eps: epsilon (input variation parameter); see https://arxiv.org/abs/1412.6572.\n",
        "    :param norm: Order of the norm (mimics NumPy). Possible values: np.inf, 1 or 2.\n",
        "    :param clip_min: (optional) float. Minimum float value for adversarial example components.\n",
        "    :param clip_max: (optional) float. Maximum float value for adversarial example components.\n",
        "    :param y: (optional) Tensor with true labels. If targeted is true, then provide the\n",
        "              target label. Otherwise, only provide this parameter if you'd like to use true\n",
        "              labels when crafting adversarial samples. Otherwise, model predictions are used\n",
        "              as labels to avoid the \"label leaking\" effect (explained in this paper:\n",
        "              https://arxiv.org/abs/1611.01236). Default is None.\n",
        "    :param sanity_checks: bool, if True, include asserts (Turn them off to use less runtime /\n",
        "              memory or for unit tests that intentionally pass strange input)\n",
        "    :return: a tensor for the adversarial example\n",
        "    \"\"\"\n",
        "    if norm not in [np.inf, 1, 2]:\n",
        "        raise ValueError(\n",
        "            \"Norm order must be either np.inf, 1, or 2, got {} instead.\".format(norm)\n",
        "        )\n",
        "    if eps < 0:\n",
        "        raise ValueError(\n",
        "            \"eps must be greater than or equal to 0, got {} instead\".format(eps)\n",
        "        )\n",
        "    if eps == 0:\n",
        "        return x\n",
        "    if clip_min is not None and clip_max is not None:\n",
        "        if clip_min > clip_max:\n",
        "            raise ValueError(\n",
        "                \"clip_min must be less than or equal to clip_max, got clip_min={} and clip_max={}\".format(\n",
        "                    clip_min, clip_max\n",
        "                )\n",
        "            )\n",
        "\n",
        "    asserts = []\n",
        "\n",
        "    # If a data range was specified, check that the input was in that range\n",
        "    if clip_min is not None:\n",
        "        assert_ge = torch.all(\n",
        "            torch.ge(x, torch.tensor(clip_min, device=x.device, dtype=x.dtype))\n",
        "        )\n",
        "        asserts.append(assert_ge)\n",
        "\n",
        "    if clip_max is not None:\n",
        "        assert_le = torch.all(\n",
        "            torch.le(x, torch.tensor(clip_max, device=x.device, dtype=x.dtype))\n",
        "        )\n",
        "        asserts.append(assert_le)\n",
        "\n",
        "    # x needs to be a leaf variable, of floating point type and have requires_grad being True for\n",
        "    # its grad to be computed and stored properly in a backward call\n",
        "    x = x.clone().detach().to(torch.float).requires_grad_(True)\n",
        "    x = x.to(active_device)\n",
        "\n",
        "    # Compute loss\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    loss = loss_fn(model_fn(x, lengths), y)\n",
        "\n",
        "    # Define gradient of loss wrt input\n",
        "    loss.backward()\n",
        "    optimal_perturbation = optimize_linear(x.grad, eps, norm)\n",
        "\n",
        "    # Add perturbation to original example to obtain adversarial example\n",
        "    adv_x = x + optimal_perturbation\n",
        "\n",
        "    # If clipping is needed, reset all values outside of [clip_min, clip_max]\n",
        "    if (clip_min is not None) or (clip_max is not None):\n",
        "        if clip_min is None or clip_max is None:\n",
        "            raise ValueError(\n",
        "                \"One of clip_min and clip_max is None but we don't currently support one-sided clipping\"\n",
        "            )\n",
        "        adv_x = torch.clamp(adv_x, clip_min, clip_max)\n",
        "\n",
        "    if sanity_checks:\n",
        "        assert np.all(asserts)\n",
        "    return adv_x"
      ],
      "metadata": {
        "id": "tNLulZAXbX3Z"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import pandas as pd\n",
        "NUM_EPOCHS = 200\n",
        "MAX_NO_IMP = 5\n",
        "MAX_VALID_LOSS = 0.145\n",
        "LOG_FILE_NAME = \"drive/MyDrive/ColabLogs/lstm_nb_adp_rt_train.txt\"\n",
        "SEED_VALUE = -1\n",
        "TRAIN_SET = \"drive/MyDrive/ColabData/rt_train_set.csv\"\n",
        "VALIDATION_SET = \"drive/MyDrive/ColabData/rt_val_set.csv\"\n",
        "TEST_SET = \"drive/MyDrive/ColabData/rt_test.csv\"\n",
        "MIN_VALID_EPOCHS = 10\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self) -> tuple:\n",
        "        print('lstm trainer - start')\n",
        "        log_file = open(LOG_FILE_NAME, \"w\", encoding=\"utf-8\")\n",
        "        seed_value = SEED_VALUE\n",
        "        set_seed(seed_value)\n",
        "            \n",
        "        train_df = pd.read_csv(TRAIN_SET)\n",
        "        val_df = pd.read_csv(VALIDATION_SET)\n",
        "        test_df = pd.read_csv(TEST_SET)\n",
        "\n",
        "        pending_model = LSTM(num_classes=train_df[LABEL_COL].unique().shape[0])\n",
        "        optimal_model = None\n",
        "        \n",
        "        optimizer =  get_optimizer(pending_model.parameters())\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        #log_soft_max = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        num_epochs = NUM_EPOCHS\n",
        "        print('start training loops. #epochs = ' + str(num_epochs))\n",
        "        print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^11} | {'Test Acc':^9} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*50)  \n",
        "        \n",
        "        log_file.write(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^11} | {'Test Acc':^9} | {'Val Acc':^9} | {'Elapsed':^9}\\n\")\n",
        "        log_file.write(\"-\"*50 + \"\\n\")\n",
        "            \n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_test_acc = 0\n",
        "        best_test_epoch = -1\n",
        "        best_epoch = -1\n",
        "        min_loss = 100\n",
        "        num_no_imp = 0\n",
        "        validation_dl = get_data_loaders(val_df, pending_model.w2v_model, sampling_type=SEQUENTIAL_SAMPLING, break_df_func=break_by_batch_size)\n",
        "        test_dl = get_data_loaders(test_df, pending_model.w2v_model, sampling_type=SEQUENTIAL_SAMPLING, break_df_func=break_by_batch_size)\n",
        "        for i in range(num_epochs):\n",
        "            epoch = i + 1\n",
        "            epoch_start_time = time.time()\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            train_dl = get_data_loaders(train_df, pending_model.w2v_model, sampling_type=SEQUENTIAL_SAMPLING, break_df_func=break_by_batch_size)\n",
        "            random.shuffle(train_dl)\n",
        "            pending_model.train()\n",
        "            for dl in train_dl:\n",
        "                for texts, labels, lengths in dl:\n",
        "                    texts = texts.to(active_device)\n",
        "                    embed_texts = pending_model.embed_text(texts)\n",
        "                    labels = labels.to(active_device)\n",
        "                    lengths = lengths.to(torch.device(\"cpu\"))\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = pending_model(embed_texts, lengths)\n",
        "                    ml_loss = loss_func(logits, labels)\n",
        "\n",
        "                    #Adversarial Training\n",
        "                    adv_texts = fast_gradient_method(pending_model, x=embed_texts, eps=5, norm=2, y=labels, lengths=lengths)\n",
        "                    adv_texts = adv_texts.to(active_device)\n",
        "                    logits = pending_model(adv_texts, lengths)\n",
        "                    ad_loss = loss_func(logits, labels)\n",
        "                    loss = ml_loss + ad_loss\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "                \n",
        "            avg_loss = total_loss / num_batches\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            \n",
        "            # Validation test.\n",
        "            val_acc, val_time = test(pending_model, validation_dl)\n",
        "            train_acc, train_time = test(pending_model, train_dl)\n",
        "            test_acc, test_time = test(pending_model, test_dl)\n",
        "            val_acc *= 100\n",
        "            train_acc *= 100\n",
        "            test_acc *= 100\n",
        "            print(f\"{epoch:^7} | {avg_loss:^12.6f}  {train_acc:^9.2f} |  {test_acc:^9.2f} |  {val_acc:^9.4f} | {epoch_time:^9.2f}\")\n",
        "            log_file.write(f\"{epoch:^7} | {avg_loss:^12.6f}  {train_acc:^9.2f} |  {test_acc:^9.2f} |  {val_acc:^9.4f} | {epoch_time:^9.2f}\\n\")\n",
        "            log_file.flush()\n",
        "                \n",
        "            if avg_loss < min_loss:\n",
        "                min_loss = avg_loss\n",
        "                num_no_imp = 0\n",
        "            else:\n",
        "                num_no_imp += 1\n",
        "                \n",
        "            if num_no_imp > MAX_NO_IMP:\n",
        "                print('early stop exit')\n",
        "                log_file.write('\\tEarly Stop exit\\n')\n",
        "                log_file.flush()\n",
        "                break\n",
        "            \n",
        "            if epoch < MIN_VALID_EPOCHS:\n",
        "                continue\n",
        "            \n",
        "            if avg_loss > MAX_VALID_LOSS:\n",
        "                continue\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                optimal_model = copy.deepcopy(pending_model)\n",
        "                best_epoch = epoch\n",
        "\n",
        "            if test_acc > best_test_acc:\n",
        "              best_test_acc = test_acc\n",
        "              best_test_epoch = epoch\n",
        "        \n",
        "        print('train_lstm_nlp - end')\n",
        "        print(\"best val: acc={:.3f}\".format(best_val_acc) + \", epoch=\" + str(best_epoch))\n",
        "        print(\"best test: acc={:.3f}\".format(best_test_acc) + \", epoch=\" + str(best_test_epoch))\n",
        "        end_train(pending_model, optimal_model, test_dl, validation_dl, log_file)\n",
        "        return pending_model, optimal_model, best_epoch\n"
      ],
      "metadata": {
        "id": "SQDiHTZMYtaH"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer()\n",
        "last_model, opt_model, best_epoch = trainer.train()\n",
        "print(best_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hF5_ISyfhxm2",
        "outputId": "d6af4d4a-f729-4d3e-d88a-bcded8176641"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm trainer - start\n",
            "Freeze embedding matrix = True\n",
            "load embedding model\n",
            "drive/MyDrive/ColabData/embed.model\n",
            "85899\n",
            "85898 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85898/85898 [00:09<00:00, 8607.30it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tw2v after padding: (85900, 256)\n",
            "generate embedding tensor\n",
            "start training loops. #epochs = 200\n",
            " Epoch  |  Train Loss  |  Train Acc  | Test Acc  |  Val Acc  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   1.425739      50.22   |    49.72   |   48.5523  |   4.02   \n",
            "   2    |   1.389760      50.22   |    49.72   |   48.5523  |   4.05   \n",
            "   3    |   1.362522      50.22   |    49.72   |   48.5523  |   4.03   \n",
            "   4    |   1.359839      50.22   |    49.72   |   48.5523  |   3.85   \n",
            "   5    |   1.383899      50.22   |    49.72   |   48.5523  |   3.90   \n",
            "   6    |   1.265456      57.46   |    59.25   |   65.9243  |   4.07   \n",
            "   7    |   0.982526      54.16   |    54.25   |   55.6793  |   3.94   \n",
            "   8    |   0.834920      72.58   |    71.62   |   71.4922  |   4.05   \n",
            "   9    |   0.705888      73.93   |    71.84   |   72.1604  |   3.98   \n",
            "  10    |   0.610671      75.42   |    74.01   |   75.0557  |   3.94   \n",
            "  11    |   0.574447      77.56   |    74.64   |   75.7238  |   4.13   \n",
            "  12    |   0.558129      79.03   |    75.90   |   76.8374  |   4.11   \n",
            "  13    |   0.511882      79.57   |    77.19   |   76.6147  |   3.96   \n",
            "  14    |   0.512770      77.44   |    75.52   |   75.0557  |   4.16   \n",
            "  15    |   0.548315      80.61   |    77.53   |   77.2829  |   3.99   \n",
            "  16    |   0.530716      81.09   |    77.91   |   78.1737  |   4.13   \n",
            "  17    |   0.505845      80.82   |    76.75   |   76.8374  |   4.20   \n",
            "  18    |   0.578421      75.00   |    73.57   |   73.2739  |   4.02   \n",
            "  19    |   0.489094      79.66   |    76.53   |   76.8374  |   4.08   \n",
            "  20    |   0.470103      82.16   |    78.07   |   79.0646  |   4.12   \n",
            "  21    |   0.468885      82.63   |    77.63   |   79.2873  |   4.04   \n",
            "  22    |   0.470953      82.83   |    78.04   |   80.1782  |   4.09   \n",
            "  23    |   0.468683      73.94   |    71.15   |   71.0468  |   4.05   \n",
            "  24    |   0.465913      81.83   |    77.69   |   78.1737  |   4.15   \n",
            "  25    |   0.457731      83.60   |    78.57   |   79.7327  |   4.28   \n",
            "  26    |   0.456376      83.82   |    78.76   |   80.1782  |   4.05   \n",
            "  27    |   0.441736      84.09   |    78.67   |   79.9555  |   4.12   \n",
            "  28    |   0.442377      84.53   |    79.17   |   80.4009  |   4.42   \n",
            "  29    |   0.448354      84.02   |    78.67   |   78.8419  |   4.10   \n",
            "  30    |   0.429589      84.79   |    79.67   |   80.1782  |   4.13   \n",
            "  31    |   0.474785      85.23   |    79.11   |   79.0646  |   4.21   \n",
            "  32    |   0.423163      84.62   |    78.07   |   78.8419  |   4.11   \n",
            "  33    |   0.411418      86.38   |    79.14   |   79.9555  |   4.34   \n",
            "  34    |   0.419367      85.81   |    79.48   |   81.5145  |   4.10   \n",
            "  35    |   0.423619      85.66   |    78.98   |   80.4009  |   4.22   \n",
            "  36    |   0.397412      87.04   |    79.70   |   79.2873  |   4.30   \n",
            "  37    |   0.512283      86.81   |    79.55   |   80.8463  |   4.13   \n",
            "  38    |   0.393396      87.05   |    79.30   |   80.1782  |   4.15   \n",
            "  39    |   0.394906      86.92   |    79.39   |   81.5145  |   4.28   \n",
            "  40    |   0.388724      87.82   |    79.30   |   79.7327  |   4.15   \n",
            "  41    |   0.378894      88.14   |    79.07   |   79.7327  |   4.27   \n",
            "  42    |   0.472162      87.99   |    79.61   |   80.8463  |   4.16   \n",
            "  43    |   0.375713      88.67   |    79.77   |   80.8463  |   4.31   \n",
            "  44    |   0.366195      81.61   |    75.24   |   75.7238  |   4.37   \n",
            "  45    |   0.363853      84.49   |    76.09   |   77.0601  |   4.19   \n",
            "  46    |   0.388608      89.64   |    79.17   |   80.8463  |   4.21   \n",
            "  47    |   0.363279      88.39   |    78.79   |   80.4009  |   4.31   \n",
            "  48    |   0.371954      89.99   |    78.92   |   81.7372  |   4.22   \n",
            "  49    |   0.390229      89.98   |    79.48   |   81.2918  |   4.40   \n",
            "  50    |   0.352336      90.35   |    79.01   |   79.7327  |   4.22   \n",
            "  51    |   0.334569      90.50   |    79.07   |   79.7327  |   4.25   \n",
            "  52    |   0.333818      90.42   |    78.73   |   79.9555  |   4.56   \n",
            "  53    |   0.330621      91.13   |    79.48   |   79.9555  |   4.21   \n",
            "  54    |   0.391059      90.23   |    78.54   |   79.2873  |   4.36   \n",
            "  55    |   0.328291      92.07   |    79.45   |   81.5145  |   4.37   \n",
            "  56    |   0.339492      91.27   |    79.23   |   81.5145  |   4.22   \n",
            "  57    |   0.324425      92.72   |    79.04   |   79.9555  |   4.26   \n",
            "  58    |   0.331211      92.57   |    79.83   |   81.7372  |   4.34   \n",
            "  59    |   0.311133      91.40   |    78.98   |   79.9555  |   4.22   \n",
            "  60    |   0.319121      92.58   |    79.14   |   81.2918  |   4.42   \n",
            "  61    |   0.298919      92.27   |    79.33   |   81.0690  |   4.27   \n",
            "  62    |   0.304410      93.03   |    79.14   |   80.8463  |   4.38   \n",
            "  63    |   0.303133      93.11   |    79.48   |   80.8463  |   4.27   \n",
            "  64    |   0.296616      93.48   |    79.58   |   81.5145  |   4.26   \n",
            "  65    |   0.317811      93.30   |    79.20   |   81.9599  |   4.48   \n",
            "  66    |   0.286150      91.03   |    78.23   |   79.2873  |   4.36   \n",
            "  67    |   0.293045      93.53   |    79.14   |   80.8463  |   4.26   \n",
            "  68    |   0.269014      93.63   |    79.36   |   80.8463  |   4.44   \n",
            "  69    |   0.270888      93.63   |    78.70   |   80.6236  |   4.38   \n",
            "  70    |   0.264138      94.77   |    79.45   |   81.9599  |   4.26   \n",
            "  71    |   0.274001      93.02   |    79.04   |   79.2873  |   4.23   \n",
            "  72    |   0.279791      94.88   |    79.33   |   81.2918  |   4.24   \n",
            "  73    |   0.259044      94.74   |    80.02   |   82.8508  |   4.60   \n",
            "  74    |   0.284475      95.55   |    79.20   |   81.5145  |   4.26   \n",
            "  75    |   0.359139      95.41   |    79.45   |   81.2918  |   4.26   \n",
            "  76    |   0.255633      95.52   |    80.02   |   81.7372  |   4.34   \n",
            "  77    |   0.255676      95.58   |    79.67   |   81.2918  |   4.37   \n",
            "  78    |   0.272297      95.59   |    79.86   |   81.2918  |   4.47   \n",
            "  79    |   0.233206      95.62   |    79.23   |   80.1782  |   4.30   \n",
            "  80    |   0.238834      94.61   |    78.73   |   79.7327  |   4.26   \n",
            "  81    |   0.245344      96.03   |    79.52   |   81.5145  |   4.57   \n",
            "  82    |   0.228663      96.32   |    79.23   |   81.9599  |   4.26   \n",
            "  83    |   0.244276      95.65   |    78.63   |   80.6236  |   4.30   \n",
            "  84    |   0.215478      96.57   |    79.23   |   81.5145  |   4.24   \n",
            "  85    |   0.276722      96.29   |    79.04   |   80.4009  |   4.41   \n",
            "  86    |   0.233610      96.47   |    79.04   |   79.2873  |   4.47   \n",
            "  87    |   0.207178      95.91   |    79.17   |   79.9555  |   4.27   \n",
            "  88    |   0.242357      96.72   |    79.23   |   81.5145  |   4.31   \n",
            "  89    |   0.212018      96.80   |    79.20   |   82.4053  |   4.44   \n",
            "  90    |   0.210753      96.96   |    78.35   |   80.6236  |   4.28   \n",
            "  91    |   0.236763      96.92   |    78.85   |   80.1782  |   4.49   \n",
            "  92    |   0.235556      97.04   |    79.42   |   81.9599  |   4.28   \n",
            "  93    |   0.190816      96.43   |    78.48   |   79.7327  |   4.26   \n",
            "  94    |   0.188150      97.64   |    79.67   |   80.4009  |   4.45   \n",
            "  95    |   0.201323      97.53   |    79.14   |   81.5145  |   4.26   \n",
            "  96    |   0.197491      97.53   |    79.58   |   80.8463  |   4.38   \n",
            "  97    |   0.217672      97.13   |    79.58   |   81.0690  |   4.27   \n",
            "  98    |   0.207474      97.53   |    78.82   |   81.2918  |   4.28   \n",
            "  99    |   0.170308      97.84   |    79.07   |   81.7372  |   4.45   \n",
            "  100   |   0.188908      97.88   |    79.01   |   82.1826  |   4.49   \n",
            "  101   |   0.227952      97.46   |    79.55   |   80.6236  |   4.27   \n",
            "  102   |   0.182648      97.91   |    78.73   |   81.2918  |   4.30   \n",
            "  103   |   0.185093      97.81   |    78.92   |   80.4009  |   4.44   \n",
            "  104   |   0.175469      97.58   |    77.56   |   80.1782  |   4.51   \n",
            "  105   |   0.208079      98.10   |    79.17   |   80.1782  |   4.29   \n",
            "early stop exit\n",
            "train_lstm_nlp - end\n",
            "best val: acc=0.000, epoch=-1\n",
            "best test: acc=0.000, epoch=-1\n",
            "Last Model\t0.79169\t0.2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-69d42cd32139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlast_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-78-0d6bdab3131c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best val: acc={:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_val_acc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", epoch=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best test: acc={:.3f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_test_acc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", epoch=\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_test_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mend_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpending_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpending_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-b129b6a78041>\u001b[0m in \u001b[0;36mend_train\u001b[0;34m(last_model, opt_model, test_dl, val_dl, log_file)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Print optimal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mopt_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-1075911c46e5>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataloaders)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mevaluated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
          ]
        }
      ]
    }
  ]
}