{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qNhvUOOoEjN",
        "outputId": "ab75f46d-13cf-4764-d42a-4e5b8bf32863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Done!\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.26.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n",
            "transformers installed!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(\"Done!\")\n",
        "\n",
        "!pip install transformers\n",
        "print('transformers installed!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_active_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "active_device = get_active_device()\n",
        "print(active_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbDHe6geotv1",
        "outputId": "8d1e8fa6-692c-4d07-b88b-8d7f346cb8a2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "TEXT_START = \"<s>\"\n",
        "TEXT_END = \"</s>\"\n",
        "PAD_LABEL = \"<pad>\"\n",
        "UNK_LABEL = \"<unk>\"\n",
        "added_words = [PAD_LABEL, UNK_LABEL, TEXT_START, TEXT_END]\n",
        "class Embedded_Words:\n",
        "    def __init__(self, model_file: str, added_pads: list, norm: bool) -> None:\n",
        "        self.vectors, self.w2i, self.i2w = self.read_model(model_file, added_pads, norm)\n",
        "\n",
        "    def read_model(self, model_file: str, added_pads: list, norm: bool) -> tuple:\n",
        "        with open(model_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [x.strip() for x in f.readlines()]\n",
        "\n",
        "        print(model_file)\n",
        "        print(len(lines))\n",
        "        print(lines[0])\n",
        "\n",
        "        num_word, dim = [int(x) for x in lines[0].split()]\n",
        "        vectors = np.zeros((num_word + len(added_pads), dim))\n",
        "        w2i = {}\n",
        "        i2w = {}\n",
        "        for line in tqdm(lines[1:]):\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            word_index = len(w2i)\n",
        "            v = np.array([float(x) for x in tokens[1:]])\n",
        "            if norm:\n",
        "                v = v / np.linalg.norm(v)\n",
        "            vectors[word_index] = v\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "\n",
        "        for word in added_pads:\n",
        "            word_index = len(w2i)\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "        \n",
        "        return vectors, w2i, i2w\n",
        "\n",
        "model_file = \"drive/MyDrive/ColabData/embed.model\"\n",
        "embedded_words = Embedded_Words(model_file, added_words, True)\n",
        "print(\"\")\n",
        "print(embedded_words.vectors.shape)\n",
        "print(embedded_words.vectors[embedded_words.vectors.shape[0]-5:,:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "embKDYu9o1bg",
        "outputId": "ceb04527-da12-4822-9bc8-c3afb1eae4bc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/ColabData/embed.model\n",
            "97930\n",
            "97929 768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97929/97929 [00:21<00:00, 4595.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(97933, 768)\n",
            "[[-0.04353679  0.00858636 -0.00491642  0.01024623 -0.0067866  -0.01306598\n",
            "   0.03756349 -0.02654051 -0.01673417  0.04313128]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('generate embedding tensor')\n",
        "# Set the embedding module.\n",
        "embedder = nn.Embedding.from_pretrained(\n",
        "    torch.FloatTensor(embedded_words.vectors).to(active_device),\n",
        "    padding_idx = embedded_words.w2i[PAD_LABEL],\n",
        "    freeze=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uayBD3fnPzLJ",
        "outputId": "e64f9b90-2b9c-4273-fbd9-a623a27febe4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generate embedding tensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Break\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "import pandas as pd\n",
        "import copy\n",
        "import random\n",
        "\n",
        "LENGTH_COL = \"lengths\"\n",
        "TEXT_COL = \"review\"\n",
        "LABEL_COL = \"sentiment\"\n",
        "MINI_BATCH_SIZE = 4096\n",
        "MAX_TRAIN_LENGTH = 512\n",
        "NUM_EDGE_TOKENS = 2\n",
        "def break_by_batch_size(df: pd.DataFrame) -> list:\n",
        "    sorted_df = df.sort_values(by=LENGTH_COL, axis=0, ascending=False, ignore_index=True)\n",
        "    labels = sorted_df[LABEL_COL].to_list()\n",
        "    lengths = sorted_df[LENGTH_COL].to_list()\n",
        "    texts = sorted_df[TEXT_COL].to_list()\n",
        "    \n",
        "    df_list = []\n",
        "    batch_size = MINI_BATCH_SIZE\n",
        "    max_valid_length = MAX_TRAIN_LENGTH - NUM_EDGE_TOKENS\n",
        "    header = {TEXT_COL:[], LABEL_COL:[], LENGTH_COL:[]}\n",
        "    row_index = 0\n",
        "    num_rows = len(labels)\n",
        "    while row_index < num_rows:\n",
        "        num_words = 0\n",
        "        curr_df = copy.deepcopy(header)\n",
        "        while num_words < batch_size and row_index < num_rows:\n",
        "            text = texts[row_index]\n",
        "            tokens = text.split()[:max_valid_length]\n",
        "            tokens = [TEXT_END] + tokens + [TEXT_END]\n",
        "            text = \" \".join(tokens)\n",
        "            num_words += len(tokens)\n",
        "            curr_df[TEXT_COL].append(text)\n",
        "            curr_df[LABEL_COL].append(labels[row_index])\n",
        "            curr_df[LENGTH_COL].append(len(tokens))\n",
        "            row_index += 1\n",
        "        \n",
        "        df_list.append(pd.DataFrame(curr_df))\n",
        "        \n",
        "    return df_list\n",
        "RANDOM_SAMPLING = \"random_sampling\"\n",
        "SEQUENTIAL_SAMPLING = \"sequential_sampling\"\n",
        "def df_to_dataloader(df: pd.DataFrame, w2v_model: Embedded_Words, sampling_type: str) -> DataLoader:\n",
        "    sorted_df = df.sort_values(by=LENGTH_COL, axis=0, ascending=False, ignore_index=True)\n",
        "    texts = sorted_df[TEXT_COL].values.tolist()\n",
        "    labels = sorted_df[LABEL_COL].values.tolist()\n",
        "    lengths = sorted_df[LENGTH_COL].to_list()\n",
        "    max_len = sorted_df[LENGTH_COL].max()\n",
        "\n",
        "    indexed_texts = []\n",
        "    attention_masks = []\n",
        "    for text in texts:\n",
        "        tokens = text.split()\n",
        "        masks = [1] * len(tokens) + [0] * (max_len - len(tokens))\n",
        "        tokens += [PAD_LABEL] * (max_len - len(tokens))\n",
        "        ids = []\n",
        "        for word in tokens:\n",
        "            if word not in w2v_model.w2i:\n",
        "                ids.append(w2v_model.w2i[UNK_LABEL])\n",
        "            else:\n",
        "                ids.append(w2v_model.w2i[word])\n",
        "        \n",
        "        indexed_texts.append(ids)\n",
        "        attention_masks.append(masks)\n",
        "        \n",
        "    inputs, attentions, labels = tuple(torch.tensor(data) for data in [indexed_texts, attention_masks, labels])\n",
        "\n",
        "    data = TensorDataset(inputs, attentions, labels)\n",
        "    \n",
        "    if sampling_type == RANDOM_SAMPLING:\n",
        "        sampler = RandomSampler(data)\n",
        "    elif sampling_type == SEQUENTIAL_SAMPLING:\n",
        "        sampler = SequentialSampler(data)\n",
        "    else:\n",
        "        print('Wrong Sampling Type: ' + sampling_type)\n",
        "        return None\n",
        "        \n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=MINI_BATCH_SIZE)\n",
        "    return dataloader\n",
        "\n",
        "def get_data_loaders(input_df: pd.DataFrame,\n",
        "                     w2v_model: Embedded_Words,\n",
        "                     sampling_type: str) -> list:\n",
        "    input_df[LENGTH_COL] = input_df[TEXT_COL].apply(lambda x: len(x.split()))\n",
        "    df_list = break_by_batch_size(input_df)\n",
        "    dataloaders = []\n",
        "    for df in df_list:\n",
        "        dataloader = df_to_dataloader(df, w2v_model, sampling_type)\n",
        "        dataloaders.append(dataloader)\n",
        "        \n",
        "    return dataloaders\n"
      ],
      "metadata": {
        "id": "lLRJxBzpp5BR"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "def test(model: nn.Module, dataloaders: list):\n",
        "    corrects = 0\n",
        "    evaluated = 0\n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    for dl in dataloaders:\n",
        "        for texts, masks, labels in dl:\n",
        "            texts = texts.to(active_device)\n",
        "            masks = masks.to(active_device)\n",
        "            labels = labels.to(active_device)\n",
        "            embed_texts = embedder(texts)\n",
        "            with torch.no_grad():\n",
        "                logits = model(embed_texts=embed_texts, masks=masks)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            corrects += (preds == labels).sum().item()\n",
        "            evaluated += texts.shape[0]\n",
        "        \n",
        "    return (corrects / evaluated), (time.time() - start_time)"
      ],
      "metadata": {
        "id": "qvhPlcSpKFof"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def end_train(last_model: nn.Module, opt_model: nn.Module, test_dl: list, val_dl: list, log_file):\n",
        "    accuracy, run_time = test(last_model, test_dl)\n",
        "    str_acc = \"{:.5f}\".format(accuracy)\n",
        "    str_time = \"{:.1f}\".format(run_time)\n",
        "    log_file.write('Last Model\\t' + str_acc + '\\t' + str_time + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Last Model\\t' + str_acc + '\\t' + str_time)\n",
        "        \n",
        "    # Print optimal\n",
        "    opt_acc, run_time = test(opt_model, test_dl)\n",
        "    val_acc, run_time = test(opt_model, val_dl)\n",
        "\n",
        "    test_acc = \"test: {:.5f}\".format(opt_acc)\n",
        "    val_acc = \"val: {:.5f}\".format(val_acc)\n",
        "    log_file.write('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc)\n"
      ],
      "metadata": {
        "id": "KHVfcIoOKNbU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed_value: int):\n",
        "    if seed_value >= 0:\n",
        "        random.seed(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        torch.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)"
      ],
      "metadata": {
        "id": "_x2sP9vgKU-7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "RHO = 0.95\n",
        "LEARNING_RATE = 1e-7\n",
        "OPT_NAME = \"adam\"\n",
        "BETA_ONE = 0\n",
        "BETA_TWO = 0.98\n",
        "ADAM_EPS = 0.00000001\n",
        "ADADELATA_OPT = \"adadelta\"\n",
        "SGD_OPT = \"sgd\"\n",
        "ADAM_OPT = \"adam\"\n",
        "def get_optimizer(parameters):\n",
        "    optimizer = None\n",
        "    if OPT_NAME == ADADELATA_OPT:\n",
        "        optimizer = optim.Adadelta(parameters,\n",
        "                                   lr=LEARNING_RATE,\n",
        "                                   rho=RHO)\n",
        "    elif OPT_NAME == SGD_OPT:\n",
        "        optimizer = optim.SGD(parameters, LEARNING_RATE)\n",
        "    elif OPT_NAME == ADAM_OPT:\n",
        "        optimizer = optim.Adam(parameters,\n",
        "                               lr=LEARNING_RATE,\n",
        "                               betas=(BETA_ONE,BETA_TWO,),\n",
        "                               eps=ADAM_EPS)\n",
        "    else:\n",
        "        print('Wrong optimizer name: ' + OPT_NAME)\n",
        "        \n",
        "    return optimizer\n",
        "\n",
        "CROSS_ENTROP_LOSS = \"cross_entropy_loss\"\n",
        "BCE_LOSS = \"bce_loss\"\n",
        "def get_loss_function(func_name: str):\n",
        "    loss_func = None\n",
        "    if func_name == CROSS_ENTROP_LOSS:\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "    elif func_name == BCE_LOSS:\n",
        "        loss_func = nn.BCELoss()\n",
        "    else:\n",
        "        print('Wrong loss function name: ' + func_name)\n",
        "        \n",
        "    return loss_func"
      ],
      "metadata": {
        "id": "G8tkVBmLKZoU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaModel\n",
        "BERT_CONFIG = \"roberta-base\"\n",
        "BERT_LABELS2ID = {\"positive\":0, \"negative\":1}\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout: float):\n",
        "\n",
        "        super(BertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = RobertaModel.from_pretrained(\n",
        "            BERT_CONFIG,\n",
        "            label2id=BERT_LABELS2ID,\n",
        "            id2label={BERT_LABELS2ID[x]:x for x in BERT_LABELS2ID},\n",
        "            num_labels=len(BERT_LABELS2ID)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, self.bert.config.num_labels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, embed_texts, masks):\n",
        "\n",
        "        _, pooled_output = self.bert(inputs_embeds=embed_texts, attention_mask=masks, return_dict=False)\n",
        "        # [batch size, hidden dim]\n",
        "\n",
        "        dropped_out = self.dropout(pooled_output)\n",
        "\n",
        "        logits = self.relu(self.linear(dropped_out))\n",
        "        #[batch size, #classes]\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "plszHlY_Kg3K"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10\n",
        "MIN_EPOCHS_TO_STOP = 2\n",
        "MAX_NO_IMP = 2\n",
        "MAX_VALID_LOSS = 0.35\n",
        "EARLY_STOP_MAX_NO_IMP = 2\n",
        "DROPOUT = 0.5\n",
        "LOG_FILE_NAME = \"drive/MyDrive/ColabLogs/roberta_base_rt_embed_texts.txt\"\n",
        "SEED_VALUE = -1\n",
        "TRAIN_SET = \"drive/MyDrive/ColabData/rt_train_set.csv\"\n",
        "VALIDATION_SET = \"drive/MyDrive/ColabData/rt_val_set.csv\"\n",
        "TEST_SET = \"drive/MyDrive/ColabData/rt_test.csv\"\n",
        "MIN_VALID_EPOCHS = 2\n",
        "\n",
        "from transformers import RobertaTokenizer\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self) -> tuple:\n",
        "        print('bert trainer - start')\n",
        "        log_file = open(LOG_FILE_NAME, \"w\", encoding=\"utf-8\")\n",
        "        set_seed(SEED_VALUE)\n",
        "\n",
        "        print(\"Load BERT model\")\n",
        "        print(\"\\tTokenizer:\")\n",
        "        start_time = time.time()\n",
        "        #tokenizer = RobertaTokenizer.from_pretrained(BERT_CONFIG)\n",
        "        print(\"\\tload time = {:.2f}\".format(time.time() - start_time))\n",
        "\n",
        "        print(\"\\tModel:\")\n",
        "        start_time = time.time()\n",
        "        pending_model = BertClassifier(dropout=DROPOUT)\n",
        "        pending_model = pending_model.to(active_device)\n",
        "        optimal_model = None\n",
        "        print(\"\\tload time = {:.2f}\".format(time.time() - start_time))\n",
        "            \n",
        "        print(\"load data frames\")\n",
        "        train_df = pd.read_csv(TRAIN_SET)\n",
        "        val_df = pd.read_csv(VALIDATION_SET)\n",
        "        test_df = pd.read_csv(TEST_SET)\n",
        "\n",
        "        print(\"load data loaders\")\n",
        "        train_dl = get_data_loaders(train_df, embedded_words, RANDOM_SAMPLING)\n",
        "        val_dl = get_data_loaders(val_df, embedded_words, SEQUENTIAL_SAMPLING)\n",
        "        test_dl = get_data_loaders(test_df, embedded_words, SEQUENTIAL_SAMPLING)\n",
        "        \n",
        "        optimizer =  get_optimizer(pending_model.parameters())\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        loss_func = loss_func.to(active_device)\n",
        "        \n",
        "        num_epochs = NUM_EPOCHS\n",
        "        print('start training loops. #epochs = ' + str(num_epochs))\n",
        "        print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^11} | {'Test Acc':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*50)  \n",
        "        \n",
        "        log_file.write(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^11} | {'Test Acc':^10} | {'Val Acc':^9} | {'Elapsed':^9}\\n\")\n",
        "        log_file.write(\"-\"*50 + \"\\n\")\n",
        "            \n",
        "        \n",
        "        best_val_acc = 0\n",
        "        best_val_epoch = -1\n",
        "        best_test_acc = 0\n",
        "        best_test_epoch = -1\n",
        "        min_loss = 100\n",
        "        num_no_imp = 0\n",
        "        for i in range(num_epochs):\n",
        "            epoch = i + 1\n",
        "            epoch_start_time = time.time()\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            random.shuffle(train_dl)\n",
        "            pending_model.train()\n",
        "            for dl in train_dl:\n",
        "                for texts, masks, labels in dl:\n",
        "                    texts = texts.to(active_device)\n",
        "                    masks = masks.to(active_device)\n",
        "                    labels = labels.to(active_device)\n",
        "                    embed_texts = embedder(texts)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    logits = pending_model(embed_texts=embed_texts, masks=masks)\n",
        "                    loss = loss_func(logits, labels)\n",
        "                    total_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                \n",
        "            avg_loss = total_loss / num_batches\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            \n",
        "            # Validation test.\n",
        "            val_acc, _ = test(pending_model, val_dl)\n",
        "            train_acc, _ = test(pending_model, train_dl)\n",
        "            test_acc, _ = test(pending_model, test_dl)\n",
        "            val_acc *= 100\n",
        "            train_acc *= 100\n",
        "            test_acc *= 100\n",
        "            print(f\"{epoch:^7} | {avg_loss:^12.6f} | {train_acc:^9.2f} | {test_acc:^9.2f} |  {val_acc:^9.4f} | {epoch_time:^9.2f}\")\n",
        "            log_file.write(f\"{epoch:^7} | {avg_loss:^12.6f}  {train_acc:^9.2f} | {test_acc:^9.2f} |  {val_acc:^9.4f} | {epoch_time:^9.2f}\\n\")\n",
        "            log_file.flush()\n",
        "                \n",
        "            if avg_loss < min_loss:\n",
        "                min_loss = avg_loss\n",
        "                num_no_imp = 0\n",
        "            else:\n",
        "                num_no_imp += 1\n",
        "                \n",
        "            if num_no_imp > EARLY_STOP_MAX_NO_IMP and epoch > MIN_EPOCHS_TO_STOP:\n",
        "                print('early stop exit')\n",
        "                log_file.write('\\tEarly Stop exit\\n')\n",
        "                log_file.flush()\n",
        "                break\n",
        "            \n",
        "            if epoch < MIN_VALID_EPOCHS:\n",
        "                continue\n",
        "            \n",
        "            if avg_loss > MAX_VALID_LOSS:\n",
        "                continue\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                optimal_model = copy.deepcopy(pending_model)\n",
        "                best_val_epoch = epoch\n",
        "\n",
        "            if test_acc > best_test_acc:\n",
        "                best_test_acc = test_acc\n",
        "                best_test_epoch = epoch\n",
        "        \n",
        "        print('bert trainer - end')\n",
        "        print(\"Best Val Acc = {:.2f}\".format(best_val_acc) + \", Best Val Epoch = \" + str(best_val_epoch))\n",
        "        print(\"Best Test Acc = {:.2f}\".format(best_test_acc) + \", Best Test Epoch = \" + str(best_test_epoch))\n",
        "        log_file.write(\"Best Val Acc = {:.2f}\".format(best_val_acc) + \", Best Val Epoch = \" + str(best_val_epoch) + \"\\n\")\n",
        "        log_file.write(\"Best Test Acc = {:.2f}\".format(best_test_acc) + \", Best Test Epoch = \" + str(best_test_epoch) + \"\\n\")\n",
        "        end_train(pending_model, optimal_model, test_dl, val_dl, log_file)\n",
        "\n",
        "        log_file.flush()\n",
        "        log_file.close()\n",
        "\n",
        "        return pending_model, optimal_model, best_val_epoch\n"
      ],
      "metadata": {
        "id": "8sNH7CaxKjKb"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer()\n",
        "last_model, opt_model, best_epoch = trainer.train()\n",
        "print(best_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "id": "GJXzgKuRQbpA",
        "outputId": "a3d1b52e-9c3d-48ed-b138-bb371618da05"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert trainer - start\n",
            "Load BERT model\n",
            "\tTokenizer:\n",
            "\tload time = 0.00\n",
            "\tModel:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tload time = 2.18\n",
            "load data frames\n",
            "load data loaders\n",
            "start training loops. #epochs = 10\n",
            " Epoch  |  Train Loss  |  Train Acc  |  Test Acc  |  Val Acc  |  Elapsed \n",
            "--------------------------------------------------\n",
            "   1    |   0.693764   |   49.78   |   50.28   |   51.4477  |   25.29  \n",
            "   2    |   0.693395   |   49.78   |   50.28   |   51.4477  |   24.85  \n",
            "   3    |   0.693341   |   49.78   |   50.28   |   51.4477  |   24.87  \n",
            "   4    |   0.693594   |   49.78   |   50.28   |   51.4477  |   24.93  \n",
            "   5    |   0.693249   |   49.78   |   50.28   |   51.4477  |   24.83  \n",
            "   6    |   0.693196   |   49.78   |   50.28   |   51.4477  |   24.84  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-69d42cd32139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlast_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-9780a7fccbd2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     \u001b[0mnum_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}