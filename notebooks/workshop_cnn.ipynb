{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S4n2J_0cxHP"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGVfz6A2d4nK",
        "outputId": "fa6bbd5f-d58f-470b-813d-682bce652664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "val_df = pd.read_csv(\"workshop_data/rt_val_set.csv\", on_bad_lines='skip', engine=\"python\")\n",
        "train_df = pd.read_csv(\"workshop_data/rt_train_set.csv\", on_bad_lines='skip', engine=\"python\")\n",
        "test_df = pd.read_csv(\"workshop_data/rt_test.csv\", on_bad_lines='skip', engine=\"python\")\n",
        "print(\"#Train = \" + str(len(train_df)))\n",
        "print(\"#Val = \" + str(len(val_df)))\n",
        "print(\"#Test = \" + str(len(test_df)))\n",
        "test_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "7dNf6KKQg71U",
        "outputId": "da24311c-7ffa-481d-c357-f04029a42eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Train = 7035\n",
            "#Val = 449\n",
            "#Test = 3178\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                             review  sentiment\n",
              "0           0  plotless collection of moronic stunts is by fa...          0\n",
              "1           1  plays like some corny television production fr...          0\n",
              "2           2  what with all the blanket statements and dime-...          0\n",
              "3           3  according to wendigo , 'nature' loves the memb...          0\n",
              "4           4  on its own , it's not very interesting . as a ...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-99029e89-a113-431c-8f4f-2a4d0d747894\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>plotless collection of moronic stunts is by fa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>plays like some corny television production fr...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>what with all the blanket statements and dime-...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>according to wendigo , 'nature' loves the memb...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>on its own , it's not very interesting . as a ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99029e89-a113-431c-8f4f-2a4d0d747894')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-99029e89-a113-431c-8f4f-2a4d0d747894 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-99029e89-a113-431c-8f4f-2a4d0d747894');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ADADELATA_OPT = 'adadelta'\n",
        "ADAM_OPT = 'adam'\n",
        "BATCH_SIZE = \"batch_size\"\n",
        "BCE_LOSS = 'bce_loss'\n",
        "CNN_KERNELS = \"cnn_kernels\"\n",
        "CNN_OUT_CHANNELS = \"cnn_out_channels\"\n",
        "CROSS_ENTROP_LOSS = 'cross_entropy_loss'\n",
        "DROPOUT = \"dropout\"\n",
        "EARLY_STOP_MAX_NO_IMP = \"early_stop_max_no_imp\"\n",
        "EMBED_WORDS_FILE = \"embed_words_file\"\n",
        "FREEZE_EMBEDDING = \"freeze_embeding\"\n",
        "LABEL_COL = \"label_col\"\n",
        "LEARNING_RATE = \"learning_rate\"\n",
        "LENGTH_COL = \"length_col\"\n",
        "LOG_FILE_NAME = \"log_file_name\"\n",
        "LOSS_FUNCTION = \"loss_function\"\n",
        "MAX_VALID_LOSS = \"max_valid_loss\"\n",
        "MIN_FRAME_ITEMS = \"min_frame_items\"\n",
        "MIN_VALID_EPOCHS = \"min_valid_epochs\"\n",
        "NORM_EMBED_VECS = \"norm_embed_vecs\"\n",
        "NUM_LENGTH_FRAMES = \"num_length_frames\"\n",
        "OPTIMIZER_NAME = \"optimizer_func\"\n",
        "PAD_LABEL = \"pad_label\"\n",
        "RANDOM_SAMPLING = \"random_sampling\"\n",
        "SEQUENTIAL_SAMPLING = \"sequential_sampling\"\n",
        "SGD_OPT = 'sgd'\n",
        "TEST_SET = \"test_set\"\n",
        "TRAIN_SET = \"train_set\"\n",
        "SAMPLING_TYPE = \"sampling_type\"\n",
        "SEED_VALUE = \"seed_value\"\n",
        "TEXT_COL = \"text_col\"\n",
        "TOKENS_COL = \"tokens_col\"\n",
        "UNK_LABEL = \"unk_label\"\n",
        "VALIDATION_SET = \"validation_set\"\n"
      ],
      "metadata": {
        "id": "KAsXtYShj3S7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "with open(\"workshop_data/nb_config.json\", 'r', encoding='utf-8') as f:\n",
        "    config = json.load(f)\n",
        "print(config)\n",
        "\n",
        "added_words = [PAD_LABEL, UNK_LABEL]\n",
        "class Embedded_Words:\n",
        "    def __init__(self, model_file: str, added_pads: list, norm: bool) -> None:\n",
        "        self.vectors, self.w2i, self.i2w = self.read_model(model_file, added_pads, norm)\n",
        "\n",
        "    def read_model(self, model_file: str, added_pads: list, norm: bool) -> tuple:\n",
        "        with open(model_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [x.strip() for x in f.readlines()]\n",
        "\n",
        "        print(model_file)\n",
        "        print(len(lines))\n",
        "        print(lines[0])\n",
        "\n",
        "        num_word, dim = [int(x) for x in lines[0].split()]\n",
        "        vectors = np.zeros((num_word + len(added_pads), dim))\n",
        "        w2i = {}\n",
        "        i2w = {}\n",
        "        for line in tqdm(lines[1:]):\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            word_index = len(w2i)\n",
        "            v = np.array([float(x) for x in tokens[1:]])\n",
        "            if norm:\n",
        "                v = v / np.linalg.norm(v)\n",
        "            vectors[word_index] = v\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "\n",
        "        for word in added_pads:\n",
        "            word_index = len(w2i)\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "        \n",
        "        return vectors, w2i, i2w\n",
        "\n",
        "embedded_words = Embedded_Words(config[EMBED_WORDS_FILE], added_words, True)\n",
        "print(\"\")\n",
        "print(embedded_words.vectors.shape)\n",
        "print(embedded_words.vectors[embedded_words.vectors.shape[0]-5:,:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mw5l18tmUSE",
        "outputId": "6f326deb-9b9e-4446-c0e1-4ada636137a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_size': 64, 'cnn_kernels': [3, 4, 5], 'cnn_out_channels': 100, 'dropout': 0.5, 'early_stop_max_no_imp': 2, 'embed_words_file': 'workshop_data/embed.model', 'freeze_embeding': 1, 'learning_rate': 0.25, 'loss_function': 'cross_entropy_loss', 'log_file_name': 'workshop_data/cnn_imdb_nb.txt', 'min_valid_epochs': 10, 'max_valid_loss': 0.1, 'min_frame_items': 500, 'norm_embed_vecs': 1, 'num_epochs': 50, 'num_length_frames': 12, 'optimizer_func': 'adadelta', 'seed_value': -1, 'sampling_type': 'random_sampling', 'label_col': 'sentiment', 'length_col': 'lengths', 'text_col': 'review', 'tokens_col': 'tokens', 'traditional_feature_type': 'tfidf', 'traditional_max_features': 7500, 'train_set': 'workshop_data/imdb_train_set.csv', 'test_set': 'workshop_data/imdb_test.csv', 'validation_set': 'workshop_data/imdb_val_set.csv'}\n",
            "workshop_data/embed.model\n",
            "116380\n",
            "116379 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 116379/116379 [00:08<00:00, 14260.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(116381, 200)\n",
            "[[-0.01921616 -0.05748059  0.08354403 -0.13734939  0.02017608  0.118749\n",
            "  -0.10769267  0.09068727 -0.04097761  0.02756285]\n",
            " [-0.03311227 -0.03198076  0.09188014 -0.10075315 -0.00427596  0.0708071\n",
            "  -0.08297284  0.03524867  0.02613228  0.05615779]\n",
            " [ 0.00295082 -0.09751952  0.08996978 -0.10860393 -0.00678612  0.04517275\n",
            "  -0.12002896  0.08097963  0.06024373  0.05645853]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_active_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "active_device = get_active_device()\n",
        "print(active_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoIh49nAVbTm",
        "outputId": "38080725-65d4-49f1-9ae3-a58171dd84eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self,\n",
        "                 config: dict,\n",
        "                 num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        print('Freeze embedding matrix = ' + str(config[FREEZE_EMBEDDING]))\n",
        "        print('load embedding model')\n",
        "        added_words = [PAD_LABEL, UNK_LABEL]\n",
        "        self.w2v_model = Embedded_Words(\"workshop_data/embed.model\", added_words, True)\n",
        "        print('\\tw2v after padding: ' + str(self.w2v_model.vectors.shape))\n",
        "\n",
        "        print('generate embedding tensor')\n",
        "        # Set the embedding module.\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(self.w2v_model.vectors).to(active_device),\n",
        "            padding_idx = self.w2v_model.w2i[PAD_LABEL],\n",
        "            freeze=config[FREEZE_EMBEDDING])\n",
        "        \n",
        "        print('allocate convolution  layers')\n",
        "        filter_width = config[CNN_OUT_CHANNELS]\n",
        "        kernels = config[CNN_KERNELS]\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=self.w2v_model.vectors.shape[1],\n",
        "                      out_channels=filter_width,\n",
        "                      kernel_size=kernel).to(active_device)\n",
        "            for kernel in kernels\n",
        "        ])\n",
        "\n",
        "        self.fc = nn.Linear(filter_width * len(kernels), num_classes).to(active_device)\n",
        "        self.dropout = nn.Dropout(p=config[DROPOUT])\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # input_ids = [#batch size, sentence len]\n",
        "        x_embed = self.embedding(input_ids).float()\n",
        "        #x_embed = [batch size, sentence len, embed dim]\n",
        "        \n",
        "        x_reshape = x_embed.permute(0, 2, 1)\n",
        "        # x_reshape = [batch size, embed dim, sentence len]\n",
        "        \n",
        "        x_conv_list = [F.relu(conv(x_reshape)) for conv in self.convs]\n",
        "        # x_conv = [batch size, out_channel_width, sentence len - kernel size + 1]\n",
        "        \n",
        "        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2]) for x_conv in x_conv_list]\n",
        "        # x_pool = [batch size, out_channel_width, 1]\n",
        "        \n",
        "        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list], dim=1)\n",
        "        # x_fc = [batch size, out_channel_width * #filters]\n",
        "        \n",
        "        logits = self.fc(self.dropout(x_fc))\n",
        "        # logits = [batch size, #classes]\n",
        "        \n",
        "        return logits\n",
        "    \n",
        "    def get_w2v_model(self):\n",
        "        return self.w2v_model\n"
      ],
      "metadata": {
        "id": "rShVUpzYiwj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "def break_df_by_len(df: pd.DataFrame, config: dict) -> list:\n",
        "    df[config[TOKENS_COL]] = df[config[TEXT_COL]].apply(lambda x: x.split(' '))\n",
        "    df[config[LENGTH_COL]] = df[config[TOKENS_COL]].map(len)\n",
        "    occurrence2len = dict(df[config[LENGTH_COL]].value_counts())\n",
        "    occurrences = sorted(occurrence2len.items())\n",
        "    frame_len = int(len(occurrences) / config[NUM_LENGTH_FRAMES]) + 1\n",
        "    \n",
        "    df_list = []\n",
        "    start_frame = 0\n",
        "    end_frame = frame_len\n",
        "    while start_frame < len(occurrences):\n",
        "        total_occurs, end_frame = get_end_frame(start_frame, end_frame, occurrences, config)\n",
        "            \n",
        "        if end_frame > len(occurrences):\n",
        "            end_frame = len(occurrences)\n",
        "            \n",
        "        curr_df = df[df[config[LENGTH_COL]] >= occurrences[start_frame][0]]\n",
        "        curr_df = curr_df[curr_df[config[LENGTH_COL]] <= occurrences[end_frame - 1][0]]\n",
        "        \n",
        "        df_list.append(curr_df)\n",
        "        \n",
        "        start_frame = end_frame\n",
        "        end_frame += frame_len\n",
        "        \n",
        "    total_occurs = 0\n",
        "    for curr_df in df_list:\n",
        "        total_occurs += len(curr_df)\n",
        "    return df_list\n",
        "\n",
        "def get_end_frame(prev_start: int, prev_end: int, occurrences: list, config: dict) -> tuple:\n",
        "    total_occurs = sum([x[1] for x in occurrences[prev_start:prev_end]])\n",
        "    \n",
        "    if total_occurs >= config[MIN_FRAME_ITEMS]:\n",
        "        return total_occurs, prev_end\n",
        "    \n",
        "    total_left = sum([x[1] for x in occurrences[prev_start:]])\n",
        "    if total_left < 2 * config[MIN_FRAME_ITEMS]:\n",
        "        end_frame = len(occurrences)\n",
        "        return total_left, end_frame\n",
        "    \n",
        "    end_frame = prev_end\n",
        "    while total_occurs < config[MIN_FRAME_ITEMS]:\n",
        "        total_occurs += occurrences[end_frame][1]\n",
        "        end_frame += 1\n",
        "        \n",
        "    return total_occurs, (end_frame + 1)\n",
        "\n",
        "def df_to_dataloader(df: pd.DataFrame, w2v_model: Embedded_Words, config: dict, sampling_type: str) -> DataLoader:\n",
        "    texts = df[config[TOKENS_COL]].values.tolist()\n",
        "    labels = df[config[LABEL_COL]].values.tolist()\n",
        "    max_len = df[config[TOKENS_COL]].map(len).max()\n",
        "\n",
        "    indexed_texts = []\n",
        "    for sentence in texts:\n",
        "        sentence += [PAD_LABEL] * (max_len - len(sentence))\n",
        "        \n",
        "        ids = [w2v_model.w2i[UNK_LABEL] if not word in w2v_model.w2i else w2v_model.w2i[word] for word in sentence]\n",
        "        indexed_texts.append(ids)\n",
        "        \n",
        "    inputs, labels = tuple(torch.tensor(data) for data in [indexed_texts, labels])\n",
        "    \n",
        "    data = TensorDataset(inputs, labels)\n",
        "    \n",
        "    if sampling_type == RANDOM_SAMPLING:\n",
        "        sampler = RandomSampler(data)\n",
        "    elif sampling_type == SEQUENTIAL_SAMPLING:\n",
        "        sampler = SequentialSampler(data)\n",
        "    else:\n",
        "        print('Wrong Sampling Type: ' + sampling_type)\n",
        "        return None\n",
        "        \n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=config[BATCH_SIZE])\n",
        "    return dataloader\n",
        "    \n",
        "\n",
        "def get_data_loaders(input_df: pd.DataFrame, w2v_model: Embedded_Words, config: dict, sampling_type: str) -> list:\n",
        "    df_list = break_df_by_len(input_df, config)\n",
        "    dataloaders = []\n",
        "    for df in df_list:\n",
        "        dataloader = df_to_dataloader(df, w2v_model, config, sampling_type)\n",
        "        dataloaders.append(dataloader)\n",
        "        \n",
        "    return dataloaders\n",
        "\n",
        "RHO = 0.95\n",
        "import torch.optim as optim\n",
        "def get_optimizer(opt_name: str, parameters, lr):\n",
        "    optimizer = None\n",
        "    if opt_name == ADADELATA_OPT:\n",
        "        optimizer = optim.Adadelta(parameters,\n",
        "                                   lr=lr,\n",
        "                                   rho=RHO)\n",
        "    elif opt_name == SGD_OPT:\n",
        "        optimizer = optim.SGD(parameters, lr)\n",
        "    elif opt_name == ADAM_OPT:\n",
        "        optimizer = optim.Adam(parameters, lr=lr)\n",
        "    else:\n",
        "        print('Wrong optimizer name: ' + opt_name)\n",
        "        \n",
        "    return optimizer\n",
        "    \n",
        "def get_loss_function(func_name: str):\n",
        "    loss_func = None\n",
        "    if func_name == CROSS_ENTROP_LOSS:\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "    elif func_name == BCE_LOSS:\n",
        "        loss_func = nn.BCELoss()\n",
        "    else:\n",
        "        print('Wrong loss function name: ' + func_name)\n",
        "        \n",
        "    return loss_func\n",
        "\n"
      ],
      "metadata": {
        "id": "gBfyDzWHugqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "\n",
        "def cnn_test(model: nn.Module, test_dl: list) -> tuple:\n",
        "    corrects = 0\n",
        "    evaluated = 0\n",
        "    t0 = time.time()\n",
        "    model.eval()\n",
        "    for dataloader in test_dl:\n",
        "        for step, data in enumerate(dataloader):\n",
        "            ids, labels = data\n",
        "            ids = ids.to(active_device)\n",
        "            labels = labels.to(active_device)\n",
        "            with torch.no_grad():\n",
        "                logits = model(ids)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            corrects += (preds == labels).sum().item()\n",
        "            evaluated += ids.shape[0]\n",
        "        \n",
        "    accuracy = corrects / evaluated\n",
        "    run_time = time.time() - t0\n",
        "    return accuracy, run_time\n"
      ],
      "metadata": {
        "id": "P0lDC4qluQBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def end_train(last_model: nn.Module, opt_model: nn.Module, config: dict, test_dl, val_dl, log_file):\n",
        "    accuracy, run_time = cnn_test(last_model, test_dl)\n",
        "    str_acc = \"{:.5f}\".format(accuracy)\n",
        "    str_time = \"{:.0f}\".format(run_time)\n",
        "    log_file.write('Last Model\\t' + str_acc + '\\t' + str_time + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Last Model\\t' + str_acc + '\\t' + str_time)\n",
        "        \n",
        "    # Print optimal\n",
        "    opt_acc, run_time = cnn_test(opt_model, test_dl)\n",
        "    val_acc, run_time = cnn_test(opt_model, val_dl)\n",
        "\n",
        "    test_acc = \"test: {:.5f}\".format(opt_acc)\n",
        "    val_acc = \"val: {:.5f}\".format(val_acc)\n",
        "    log_file.write('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc)\n",
        "    log_file.write(\"\\nConfiguraton:\\n\")\n",
        "    sorted_params = sorted(config.items())\n",
        "    for param in sorted_params:\n",
        "        log_file.write(param[0] + \"\\t\" + str(param[1]) + \"\\n\")\n",
        "        log_file.flush()\n",
        "\n",
        "import random\n",
        "def set_seed(seed_value: int):\n",
        "    if seed_value >= 0:\n",
        "        random.seed(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        torch.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "import copy\n",
        "NUM_EPOCHS = 100\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self, config: dict) -> tuple:\n",
        "        print('cnn trainer - start')\n",
        "        log_file = open(config[LOG_FILE_NAME], \"w\", encoding=\"utf-8\")\n",
        "        seed_value = config[SEED_VALUE]\n",
        "        set_seed(seed_value)\n",
        "\n",
        "        train_df = pd.read_csv(config[TRAIN_SET])\n",
        "        val_df = pd.read_csv(config[VALIDATION_SET])\n",
        "        test_df = pd.read_csv(config[TEST_SET])\n",
        "        pending_model = CNN(config, train_df[config[LABEL_COL]].unique().shape[0])\n",
        "        optimal_model = None\n",
        "\n",
        "        train_dl = get_data_loaders(train_df, pending_model.get_w2v_model(), config, config[SAMPLING_TYPE])\n",
        "        val_dl = get_data_loaders(val_df, pending_model.get_w2v_model(), config, config[SAMPLING_TYPE])\n",
        "        test_dl = get_data_loaders(test_df, pending_model.get_w2v_model(), config, config[SAMPLING_TYPE])\n",
        "        \n",
        "        print('set optimizer & loss')\n",
        "        best_val_acc = 0\n",
        "        best_epoch = -1\n",
        "        optimizer = get_optimizer(config[OPTIMIZER_NAME], pending_model.parameters(), config[LEARNING_RATE])\n",
        "        loss_func = get_loss_function(config[LOSS_FUNCTION])\n",
        "        \n",
        "        print('get data loaders')\n",
        "        sampling = RANDOM_SAMPLING if seed_value < 0 else SEQUENTIAL_SAMPLING\n",
        "        print('\\tsampling = ' + sampling)\n",
        "        \n",
        "        num_epochs = NUM_EPOCHS\n",
        "        print('start training loops. #epochs = ' + str(num_epochs))\n",
        "        print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^9} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*30)  \n",
        "        \n",
        "        log_file.write(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^9} | {'Val Acc':^9} | {'Elapsed':^9}\\n\")\n",
        "        log_file.write(\"-\"*30 + '\\n')  \n",
        "        log_file.flush()\n",
        "            \n",
        "        \n",
        "        min_loss = 100\n",
        "        num_no_imp = 0\n",
        "        for i in range(num_epochs):\n",
        "            epoch = i + 1\n",
        "            epoch_start_time = time.time()\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "            \n",
        "            pending_model.train()\n",
        "            \n",
        "            for dataloader in train_dl:\n",
        "                for step, batch in enumerate(dataloader):\n",
        "                    ids, labels = tuple(t for t in batch)\n",
        "                    ids = ids.to(active_device)\n",
        "                    labels = labels.to(active_device)\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    \n",
        "                    logits = pending_model(ids)\n",
        "                    \n",
        "                    loss = loss_func(logits, labels)\n",
        "                    total_loss += loss.item()\n",
        "                    num_batches += 1\n",
        "                    \n",
        "                    loss.backward()\n",
        "                    \n",
        "                    optimizer.step()\n",
        "                \n",
        "            avg_loss = total_loss / num_batches\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            \n",
        "            # Validation test.\n",
        "            val_acc, val_time = cnn_test(pending_model, val_dl)\n",
        "            train_acc, val_time = cnn_test(pending_model, train_dl)\n",
        "            val_acc *= 100\n",
        "            train_acc *= 100\n",
        "            print(f\"{epoch:^7} | {avg_loss:^12.6f} | {train_acc:^9.2f} | {val_acc:^9.2f} | {epoch_time:^9.2f}\")\n",
        "            log_file.write(f\"{epoch:^7} | {avg_loss:^12.6f} | {train_acc:^9.2f} | {val_acc:^9.2f} | {epoch_time:^9.2f}\\n\")\n",
        "            log_file.flush()\n",
        "                \n",
        "            if avg_loss < min_loss:\n",
        "                min_loss = avg_loss\n",
        "                num_no_imp = 0\n",
        "            else:\n",
        "                num_no_imp += 1\n",
        "                \n",
        "            if num_no_imp > config[EARLY_STOP_MAX_NO_IMP]:\n",
        "                print('early stop exit')\n",
        "                log_file.write('\\tEarly Stop exit\\n')\n",
        "                log_file.flush()\n",
        "                break\n",
        "            \n",
        "            if epoch < config[MIN_VALID_EPOCHS]:\n",
        "                continue\n",
        "            \n",
        "            if avg_loss > config[MAX_VALID_LOSS]:\n",
        "                continue\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                optimal_model = copy.deepcopy(pending_model)\n",
        "                best_epoch = epoch\n",
        "        \n",
        "        print('train_cnn_nlp - end')\n",
        "        end_train(pending_model, optimal_model, config, test_dl, val_dl, log_file)\n",
        "        log_file.close()\n",
        "        return pending_model, optimal_model, best_epoch\n"
      ],
      "metadata": {
        "id": "6D3LptZky9Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"workshop_data/nb_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  actual_config = json.load(f)\n",
        "\n",
        "trainer = Trainer()\n",
        "last_model, opt_model, best_epoch = trainer.train(actual_config)\n",
        "print(best_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8mMt3bVjdF7",
        "outputId": "e1339305-cff8-4ee9-9743-fd96bc85a829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn trainer - start\n",
            "Freeze embedding matrix = 1\n",
            "load embedding model\n",
            "workshop_data/embed.model\n",
            "116380\n",
            "116379 200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 116379/116379 [00:09<00:00, 12443.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tw2v after padding: (116381, 200)\n",
            "generate embedding tensor\n",
            "allocate convolution  layers\n",
            "set optimizer & loss\n",
            "get data loaders\n",
            "\tsampling = random_sampling\n",
            "start training loops. #epochs = 100\n",
            " Epoch  |  Train Loss  | Train Acc |  Val Acc  |  Elapsed \n",
            "------------------------------\n",
            "   1    |   0.686667   |   50.26   |   48.55   |   0.42   \n",
            "   2    |   0.662800   |   74.75   |   69.93   |   0.40   \n",
            "   3    |   0.599557   |   71.34   |   68.37   |   0.41   \n",
            "   4    |   0.529423   |   79.60   |   76.61   |   0.40   \n",
            "   5    |   0.483931   |   80.27   |   75.50   |   0.43   \n",
            "   6    |   0.462344   |   82.06   |   76.17   |   0.40   \n",
            "   7    |   0.437107   |   77.95   |   73.05   |   0.43   \n",
            "   8    |   0.413158   |   83.07   |   75.72   |   0.41   \n",
            "   9    |   0.396882   |   85.05   |   75.95   |   0.52   \n",
            "  10    |   0.381708   |   84.82   |   75.95   |   0.56   \n",
            "  11    |   0.360405   |   83.31   |   74.16   |   0.55   \n",
            "  12    |   0.358750   |   86.91   |   76.17   |   0.41   \n",
            "  13    |   0.335789   |   82.22   |   73.27   |   0.42   \n",
            "  14    |   0.325384   |   85.94   |   74.61   |   0.41   \n",
            "  15    |   0.314452   |   87.42   |   75.28   |   0.41   \n",
            "  16    |   0.295614   |   85.02   |   73.94   |   0.41   \n",
            "  17    |   0.276365   |   93.06   |   77.28   |   0.40   \n",
            "  18    |   0.267979   |   92.22   |   77.06   |   0.43   \n",
            "  19    |   0.262350   |   93.82   |   77.06   |   0.41   \n",
            "  20    |   0.244155   |   84.48   |   74.39   |   0.40   \n",
            "  21    |   0.244478   |   95.79   |   77.95   |   0.42   \n",
            "  22    |   0.215300   |   87.19   |   71.05   |   0.41   \n",
            "  23    |   0.210129   |   96.90   |   77.06   |   0.41   \n",
            "  24    |   0.205712   |   97.13   |   77.28   |   0.41   \n",
            "  25    |   0.189763   |   93.80   |   75.28   |   0.40   \n",
            "  26    |   0.200075   |   98.01   |   76.61   |   0.45   \n",
            "  27    |   0.166445   |   94.44   |   76.39   |   0.41   \n",
            "  28    |   0.163403   |   98.65   |   75.72   |   0.51   \n",
            "  29    |   0.162167   |   98.82   |   77.06   |   0.54   \n",
            "  30    |   0.144508   |   98.92   |   76.84   |   0.55   \n",
            "  31    |   0.127179   |   98.81   |   75.95   |   0.40   \n",
            "  32    |   0.134564   |   99.05   |   75.95   |   0.41   \n",
            "  33    |   0.125663   |   98.17   |   75.50   |   0.41   \n",
            "  34    |   0.124397   |   99.20   |   77.51   |   0.43   \n",
            "  35    |   0.113951   |   98.93   |   77.06   |   0.41   \n",
            "  36    |   0.104602   |   80.90   |   71.94   |   0.40   \n",
            "  37    |   0.089842   |   99.28   |   76.61   |   0.41   \n",
            "  38    |   0.095877   |   99.74   |   77.51   |   0.42   \n",
            "  39    |   0.094621   |   99.77   |   77.06   |   0.41   \n",
            "  40    |   0.083390   |   97.38   |   71.05   |   0.62   \n",
            "  41    |   0.074019   |   98.93   |   77.28   |   0.58   \n",
            "  42    |   0.067020   |   99.69   |   76.61   |   0.41   \n",
            "  43    |   0.066543   |   99.77   |   76.84   |   0.41   \n",
            "  44    |   0.068916   |   99.90   |   77.51   |   0.41   \n",
            "  45    |   0.074293   |   99.91   |   77.06   |   0.52   \n",
            "  46    |   0.053086   |   99.91   |   77.06   |   0.54   \n",
            "  47    |   0.061006   |   77.74   |   70.38   |   0.56   \n",
            "  48    |   0.050774   |   99.93   |   76.39   |   0.41   \n",
            "  49    |   0.045567   |   99.90   |   77.06   |   0.42   \n",
            "  50    |   0.044251   |   99.94   |   77.51   |   0.41   \n",
            "  51    |   0.077845   |   99.91   |   76.84   |   0.41   \n",
            "  52    |   0.039346   |   99.57   |   76.84   |   0.41   \n",
            "  53    |   0.042213   |   99.96   |   77.51   |   0.42   \n",
            "  54    |   0.036645   |   99.93   |   76.39   |   0.43   \n",
            "  55    |   0.034252   |   99.96   |   77.73   |   0.41   \n",
            "  56    |   0.036841   |   90.56   |   67.93   |   0.41   \n",
            "  57    |   0.036129   |   99.97   |   77.28   |   0.40   \n",
            "  58    |   0.032830   |   99.97   |   77.51   |   0.41   \n",
            "  59    |   0.029538   |   99.97   |   77.28   |   0.40   \n",
            "  60    |   0.031192   |   99.97   |   76.17   |   0.42   \n",
            "  61    |   0.031678   |   99.97   |   77.28   |   0.40   \n",
            "  62    |   0.025022   |   99.97   |   77.28   |   0.41   \n",
            "  63    |   0.025129   |   99.97   |   77.06   |   0.42   \n",
            "  64    |   0.022353   |   99.97   |   77.28   |   0.56   \n",
            "  65    |   0.022132   |   99.97   |   77.28   |   0.53   \n",
            "  66    |   0.022760   |   99.96   |   75.28   |   0.49   \n",
            "  67    |   0.022592   |   99.96   |   77.95   |   0.42   \n",
            "  68    |   0.023516   |   99.99   |   77.95   |   0.41   \n",
            "early stop exit\n",
            "train_cnn_nlp - end\n",
            "Last Model\t0.78697\t0\n",
            "Optimal Model\tTest=test: 0.78603\tVal=val: 0.77951\n",
            "67\n"
          ]
        }
      ]
    }
  ]
}