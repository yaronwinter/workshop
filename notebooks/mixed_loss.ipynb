{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV6_S8Z3Z3A-",
        "outputId": "b2d27f6c-064b-426b-9c3e-b05a532d3e63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "UNK_LABEL = \"unk_label\"\n",
        "PAD_LABEL = \"pad_label\"\n",
        "added_words = [PAD_LABEL, UNK_LABEL]\n",
        "class Embedded_Words:\n",
        "    def __init__(self, model_file: str, added_pads: list, norm: bool) -> None:\n",
        "        self.vectors, self.w2i, self.i2w = self.read_model(model_file, added_pads, norm)\n",
        "\n",
        "    def read_model(self, model_file: str, added_pads: list, norm: bool) -> tuple:\n",
        "        with open(model_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = [x.strip() for x in f.readlines()]\n",
        "\n",
        "        print(model_file)\n",
        "        print(len(lines))\n",
        "        print(lines[0])\n",
        "\n",
        "        num_word, dim = [int(x) for x in lines[0].split()]\n",
        "        vectors = np.zeros((num_word + len(added_pads), dim))\n",
        "        w2i = {}\n",
        "        i2w = {}\n",
        "        for line in tqdm(lines[1:]):\n",
        "            tokens = line.split()\n",
        "            word = tokens[0]\n",
        "            word_index = len(w2i)\n",
        "            v = np.array([float(x) for x in tokens[1:]])\n",
        "            if norm:\n",
        "                v = v / np.linalg.norm(v)\n",
        "            vectors[word_index] = v\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "\n",
        "        for word in added_pads:\n",
        "            word_index = len(w2i)\n",
        "            w2i[word] = word_index\n",
        "            i2w[word_index] = word\n",
        "        \n",
        "        return vectors, w2i, i2w\n",
        "\n",
        "model_file = \"drive/MyDrive/ColabData/embed.model\"\n",
        "embedded_words = Embedded_Words(model_file, added_words, True)\n",
        "print(\"\")\n",
        "print(embedded_words.vectors.shape)\n",
        "print(embedded_words.vectors[embedded_words.vectors.shape[0]-5:,:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFRTcPq8aLBG",
        "outputId": "0e3426c2-3089-4d0e-c3a8-6ea99d561520"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive/MyDrive/ColabData/embed.model\n",
            "85899\n",
            "85898 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85898/85898 [00:08<00:00, 9595.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "(85900, 256)\n",
            "[[ 0.06298662 -0.03220133 -0.02866318  0.12042718  0.18225189 -0.06114065\n",
            "   0.05848268 -0.04147663  0.04435284 -0.00816747]\n",
            " [ 0.05723497 -0.02765993  0.0106675   0.13086651  0.09357034 -0.08705442\n",
            "  -0.00878455 -0.0667807  -0.00667503 -0.00585762]\n",
            " [-0.05147354 -0.00427562  0.09155177  0.13254647  0.11083994 -0.00598478\n",
            "   0.02139013 -0.03059529 -0.08231317  0.02991129]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]\n",
            " [ 0.          0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_active_device():\n",
        "    \"\"\"Picking GPU if available or else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "active_device = get_active_device()\n",
        "print(active_device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WINSOE8NakL7",
        "outputId": "c6cb7d10-070c-4597-8c55-1b991edb7c6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "\n",
        "BATCH_FIRST = True\n",
        "FREEZE_EMBEDDING = True\n",
        "NORM_EMBED_VECS = True\n",
        "HIDDEN_DIM = 512\n",
        "BIDIRECTIONAL = True\n",
        "NUM_LAYERS = 1\n",
        "DROPOUT = 0.5\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes: int):\n",
        "        # Constructor.\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        print('Freeze embedding matrix = ' + str(FREEZE_EMBEDDING))\n",
        "        print('load embedding model')\n",
        "        added_words = [PAD_LABEL, UNK_LABEL]\n",
        "        self.w2v_model = Embedded_Words(model_file, added_words, NORM_EMBED_VECS)\n",
        "        print('\\tw2v after padding: ' + str(self.w2v_model.vectors.shape))\n",
        "\n",
        "        print('generate embedding tensor')\n",
        "        # Set the embedding module.\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(self.w2v_model.vectors).to(active_device),\n",
        "            padding_idx = self.w2v_model.w2i[PAD_LABEL],\n",
        "            freeze=True) #config[FREEZE_EMBEDDING])\n",
        "        \n",
        "        # LSTM layer.\n",
        "        hidden_dim = HIDDEN_DIM\n",
        "        self.lstm = nn.LSTM(self.w2v_model.vectors.shape[1],\n",
        "                            hidden_dim,\n",
        "                            num_layers=NUM_LAYERS,\n",
        "                            bidirectional=BIDIRECTIONAL,\n",
        "                            dropout=0,\n",
        "                            batch_first=BATCH_FIRST).to(active_device)\n",
        "        \n",
        "        # Set the dropout for the embedding layer and the lstm's output layer.\n",
        "        self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "        # Set the last layer, fully connected.\n",
        "        self.fc = nn.Linear(hidden_dim * (2 if BIDIRECTIONAL else 1), num_classes).to(active_device)\n",
        "\n",
        "    def embed_text(self, texts):\n",
        "      return self.dropout(self.embedding(texts))\n",
        "        \n",
        "    # The forward function.\n",
        "    def forward(self, texts, lengths):\n",
        "        # Get the embedding of the given text.\n",
        "        # texts = [#batch size, sentence length, embed dim]\n",
        "        #lengths = [#batch size]\n",
        "        #embed_text = self.dropout(self.embedding(texts))\n",
        "        # embed_text = [#batch size, sentence length, embed dim]\n",
        "        \n",
        "        # Get the packed sentences.\n",
        "        packed_text = pack_padded_sequence(texts, lengths, batch_first=BATCH_FIRST)\n",
        "        # packed_text = [[sum lengths, embed dim], [#sequence length (#active batch items for ech length)]]\n",
        "\n",
        "        # Call the LSTM layer.\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_text)\n",
        "        # packed output = [[sum lengths, hidden dim], [#sequence length]]\n",
        "        # hidden = [1 (2 if bidirectional), #batch size, hidden dim]\n",
        "        # cell   = [1 (2 if bidirectional), #batch size, hidden dim]\n",
        "        \n",
        "        # unpack the output.\n",
        "        pad_packed_output, _ = pad_packed_sequence(packed_output, batch_first=BATCH_FIRST)\n",
        "        # pad_packed_output = [batch size, sentence length, hidden dim * 2]\n",
        "        \n",
        "        # Prmute the output before pooling.\n",
        "        permuted_output = self.dropout(pad_packed_output.permute(0, 2, 1))\n",
        "        # permuted_output = [batch size, hidden dim * 2, sentence length]\n",
        "\n",
        "        # Max pooling layer.        \n",
        "        pooled_output = F.max_pool1d(permuted_output, kernel_size=permuted_output.shape[2])\n",
        "        # pooled_output = [batch size, hidden dim * 2, 1]\n",
        "        \n",
        "        # Call the linear full connected layer, after droping out.\n",
        "        logits = self.fc(torch.squeeze(pooled_output, dim=2))\n",
        "        # logits = [batch size, #classes]\n",
        "        \n",
        "        return logits"
      ],
      "metadata": {
        "id": "4vdczLVpaoup"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import RandomSampler\n",
        "from torch.utils.data import SequentialSampler\n",
        "from torch.utils.data import TensorDataset\n",
        "import pandas as pd\n",
        "import copy\n",
        "import random\n",
        "\n",
        "LENGTH_COL = \"lengths\"\n",
        "TOKENS_COL = \"tokens\"\n",
        "LABEL_COL = \"sentiment\"\n",
        "MINI_BATCH_SIZE = 3000\n",
        "MAX_TRAIN_LENGTH = 512\n",
        "def break_by_batch_size(df: pd.DataFrame) -> list:\n",
        "    sorted_df = df.sort_values(by=LENGTH_COL, axis=0, ascending=False, ignore_index=True)\n",
        "    tokens = sorted_df[TOKENS_COL].to_list()\n",
        "    labels = sorted_df[LABEL_COL].to_list()\n",
        "    lengths = sorted_df[LENGTH_COL].to_list()\n",
        "    \n",
        "    df_list = []\n",
        "    header = {TOKENS_COL:[], LABEL_COL:[], LENGTH_COL:[]}\n",
        "    row_index = 0\n",
        "    num_rows = len(labels)\n",
        "    while row_index < num_rows:\n",
        "        num_words = 0\n",
        "        curr_df = copy.deepcopy(header)\n",
        "        while num_words < MINI_BATCH_SIZE and row_index < num_rows:\n",
        "            actual_length = min(MAX_TRAIN_LENGTH, lengths[row_index])\n",
        "            num_words += actual_length\n",
        "            curr_df[TOKENS_COL].append(tokens[row_index][:MAX_TRAIN_LENGTH])\n",
        "            curr_df[LABEL_COL].append(labels[row_index])\n",
        "            curr_df[LENGTH_COL].append(actual_length)\n",
        "            row_index += 1\n",
        "        \n",
        "        df_list.append(pd.DataFrame(curr_df))\n",
        "        \n",
        "    return df_list\n",
        "\n",
        "DROP_WORD_PROB = -1\n",
        "RANDOM_SAMPLING = \"random_sampling\"\n",
        "SEQUENTIAL_SAMPLING = \"sequential_sampling\"\n",
        "def df_to_dataloader(df: pd.DataFrame, w2v_model: Embedded_Words, sampling_type: str, is_labeled: int) -> DataLoader:\n",
        "    sorted_df = df.sort_values(by=LENGTH_COL, axis=0, ascending=False, ignore_index=True)\n",
        "    texts = sorted_df[TOKENS_COL].values.tolist()\n",
        "    labels = sorted_df[LABEL_COL].values.tolist()\n",
        "    lengths = sorted_df[LENGTH_COL].to_list()\n",
        "    max_len = sorted_df[TOKENS_COL].map(len).max()\n",
        "    labeled = [is_labeled] * len(df)\n",
        "\n",
        "    indexed_texts = []\n",
        "    for sentence in texts:\n",
        "        sentence += [PAD_LABEL] * (max_len - len(sentence))\n",
        "        ids = []\n",
        "        for word in sentence:\n",
        "            if word not in w2v_model.w2i:\n",
        "                ids.append(w2v_model.w2i[UNK_LABEL])\n",
        "            elif np.random.random() < DROP_WORD_PROB:\n",
        "                ids.append(w2v_model.w2i[UNK_LABEL])\n",
        "            else:\n",
        "                ids.append(w2v_model.w2i[word])\n",
        "        \n",
        "        indexed_texts.append(ids)\n",
        "        \n",
        "    inputs, labels, lengths, labeled = tuple(torch.tensor(data) for data in [indexed_texts, labels, lengths, labeled])\n",
        "\n",
        "    data = TensorDataset(inputs, labels, lengths, labeled)\n",
        "    \n",
        "    if sampling_type == RANDOM_SAMPLING:\n",
        "        sampler = RandomSampler(data)\n",
        "    elif sampling_type == SEQUENTIAL_SAMPLING:\n",
        "        sampler = SequentialSampler(data)\n",
        "    else:\n",
        "        print('Wrong Sampling Type: ' + sampling_type)\n",
        "        return None\n",
        "        \n",
        "    dataloader = DataLoader(data, sampler=sampler, batch_size=MINI_BATCH_SIZE)\n",
        "    return dataloader\n",
        "\n",
        "TEXT_COL = \"review\"\n",
        "def get_data_loaders(input_df: pd.DataFrame,\n",
        "                     w2v_model: Embedded_Words,\n",
        "                     sampling_type: str,\n",
        "                     is_labeled: int,\n",
        "                     break_df_func) -> list:\n",
        "    input_df[TOKENS_COL] = input_df[TEXT_COL].apply(lambda x: x.split(' '))\n",
        "    input_df[LENGTH_COL] = input_df[TOKENS_COL].map(len)\n",
        "    df_list = break_df_func(input_df)\n",
        "    dataloaders = []\n",
        "    for df in df_list:\n",
        "        dataloader = df_to_dataloader(df, w2v_model, sampling_type, is_labeled)\n",
        "        dataloaders.append(dataloader)\n",
        "        \n",
        "    return dataloaders\n",
        "\n",
        "RHO = 0.95\n",
        "LEARNING_RATE = 0.001\n",
        "OPT_NAME = \"adam\"\n",
        "BETA_ONE = 0\n",
        "BETA_TWO = 0.98\n",
        "ADAM_EPS = 0.00000001\n",
        "ADADELATA_OPT = \"adadelta\"\n",
        "SGD_OPT = \"sgd\"\n",
        "ADAM_OPT = \"adam\"\n",
        "def get_optimizer(parameters):\n",
        "    optimizer = None\n",
        "    if OPT_NAME == ADADELATA_OPT:\n",
        "        optimizer = optim.Adadelta(parameters,\n",
        "                                   lr=LEARNING_RATE,\n",
        "                                   rho=RHO)\n",
        "    elif OPT_NAME == SGD_OPT:\n",
        "        optimizer = optim.SGD(parameters, LEARNING_RATE)\n",
        "    elif OPT_NAME == ADAM_OPT:\n",
        "        optimizer = optim.Adam(parameters,\n",
        "                               lr=LEARNING_RATE,\n",
        "                               betas=(BETA_ONE,BETA_TWO,),\n",
        "                               eps=ADAM_EPS)\n",
        "    else:\n",
        "        print('Wrong optimizer name: ' + OPT_NAME)\n",
        "        \n",
        "    return optimizer\n",
        "    \n",
        "\n",
        "CROSS_ENTROP_LOSS = \"cross_entropy_loss\"\n",
        "BCE_LOSS = \"bce_loss\"\n",
        "def get_loss_function(func_name: str):\n",
        "    loss_func = None\n",
        "    if func_name == CROSS_ENTROP_LOSS:\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "    elif func_name == BCE_LOSS:\n",
        "        loss_func = nn.BCELoss()\n",
        "    else:\n",
        "        print('Wrong loss function name: ' + func_name)\n",
        "        \n",
        "    return loss_func\n",
        "\n",
        "\n",
        "def set_seed(seed_value: int):\n",
        "    if seed_value >= 0:\n",
        "        random.seed(seed_value)\n",
        "        np.random.seed(seed_value)\n",
        "        torch.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value)\n"
      ],
      "metadata": {
        "id": "rc2naaeJauxH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def test(model: nn.Module, dataloaders: list):\n",
        "    corrects = 0\n",
        "    evaluated = 0\n",
        "    start_time = time.time()\n",
        "    model.eval()\n",
        "    for dl in dataloaders:\n",
        "        for texts, labels, lengths, _ in dl:\n",
        "            texts = texts.to(active_device)\n",
        "            labels = labels.to(active_device)\n",
        "            lengths = lengths.to(torch.device(\"cpu\"))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embed_text = model.embed_text(texts)\n",
        "                logits = model(embed_text, lengths)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "            corrects += (preds == labels).sum().item()\n",
        "            evaluated += texts.shape[0]\n",
        "        \n",
        "    accuracy = corrects / evaluated\n",
        "    run_time = time.time() - start_time\n",
        "    return accuracy, run_time"
      ],
      "metadata": {
        "id": "jv4HWTpBa11Y"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def end_train(last_model: nn.Module, opt_model: nn.Module, test_dl: list, val_dl: list, log_file):\n",
        "    accuracy, run_time = test(last_model, test_dl)\n",
        "    str_acc = \"{:.5f}\".format(accuracy)\n",
        "    str_time = \"{:.1f}\".format(run_time)\n",
        "    log_file.write('Last Model\\t' + str_acc + '\\t' + str_time + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Last Model\\t' + str_acc + '\\t' + str_time)\n",
        "        \n",
        "    # Print optimal\n",
        "    opt_acc, run_time = test(opt_model, test_dl)\n",
        "    val_acc, run_time = test(opt_model, val_dl)\n",
        "\n",
        "    test_acc = \"test: {:.5f}\".format(opt_acc)\n",
        "    val_acc = \"val: {:.5f}\".format(val_acc)\n",
        "    log_file.write('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc + '\\n')\n",
        "    log_file.flush()\n",
        "    print('Optimal Model\\tTest=' + test_acc + '\\tVal=' + val_acc)\n",
        "    log_file.write(\"\\nConfiguraton:\\n\")\n"
      ],
      "metadata": {
        "id": "LSCL4hypbAZ2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPS_ADV = 0.1\n",
        "VAT_EPS = 5\n",
        "\n",
        "class EMLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EMLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
        "        b = -1.0 * b.sum()\n",
        "        b = b / x.shape[0]\n",
        "        return b\n",
        "\n",
        "def generate_vat_texts(embed_texts: torch.Tensor) -> torch.Tensor:\n",
        "    vat_vecs = torch.normal(0, 1, size=embed_texts.size())\n",
        "    square = torch.sum(vat_vecs ** 2, [2], keepdim=True)\n",
        "    vat_texts = vat_vecs / torch.sqrt(square)\n",
        "    vat_texts = embed_texts + EPS_ADV*vat_texts\n",
        "    return vat_texts\n",
        "\n",
        "def generate_vat_loss(embed_texts: torch.Tensor, lengths: torch.Tensor, model: LSTM) -> torch.Tensor:\n",
        "    x = embed_texts.clone().detach().to(torch.float)\n",
        "    x = x.to(torch.device('cpu'))\n",
        "    vat_texts = generate_vat_texts(x)\n",
        "\n",
        "    vat_texts = vat_texts.to(active_device)\n",
        "    vat_texts.requires_grad = True\n",
        "    x = x.to(active_device)\n",
        "    vat_logits = model(vat_texts, lengths)\n",
        "    reg_logits = model(x, lengths)\n",
        "\n",
        "    reg_log_logits = F.log_softmax(reg_logits, dim=1)\n",
        "    vat_log_logits = F.log_softmax(vat_logits, dim=1)\n",
        "\n",
        "    loss = F.kl_div(vat_log_logits, reg_log_logits, reduction=\"batchmean\", log_target=True)\n",
        "    loss.backward()\n",
        "\n",
        "    grad = vat_texts.grad\n",
        "    grad = grad.clone().detach().to(torch.float)\n",
        "    square = torch.sum(grad ** 2, [2], keepdim=True)\n",
        "    norm_grad = grad / torch.sqrt(square)\n",
        "    star_vecs = embed_texts.clone().detach().to(torch.float) + VAT_EPS*norm_grad\n",
        "    star_vecs = star_vecs.to(active_device)\n",
        "    star_logits = model(star_vecs, lengths)\n",
        "    star_log_logits = F.log_softmax(star_logits, dim=1)\n",
        "\n",
        "    x = x.clone().detach().to(torch.float)\n",
        "    x = x.to(active_device)\n",
        "    reg_logits = model(x, lengths)\n",
        "    reg_log_logits = F.log_softmax(reg_logits, dim=1)\n",
        "\n",
        "    return F.kl_div(star_log_logits, reg_log_logits, reduction=\"batchmean\", log_target=True)\n"
      ],
      "metadata": {
        "id": "Mxy-EazwdHyK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import pandas as pd\n",
        "NUM_EPOCHS = 200\n",
        "MIN_EPOCHS_TO_STOP = 75\n",
        "MAX_NO_IMP = 3\n",
        "MAX_VALID_LOSS = 0.35\n",
        "LOG_FILE_NAME = \"drive/MyDrive/ColabLogs/lstm_imdb_ml_em_unlab.txt\"\n",
        "SEED_VALUE = -1\n",
        "TRAIN_SET = \"drive/MyDrive/ColabData/imdb_train_set.csv\"\n",
        "VALIDATION_SET = \"drive/MyDrive/ColabData/imdb_val_set.csv\"\n",
        "TEST_SET = \"drive/MyDrive/ColabData/imdb_test.csv\"\n",
        "UNLABELED_SET = \"drive/MyDrive/ColabData/imdb_unlabeled_set.csv\"\n",
        "MIN_VALID_EPOCHS = 10\n",
        "class Trainer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def train(self) -> tuple:\n",
        "        print('lstm trainer - start')\n",
        "        log_file = open(LOG_FILE_NAME, \"w\", encoding=\"utf-8\")\n",
        "        set_seed(SEED_VALUE)\n",
        "\n",
        "        unlabeled_df = pd.read_csv(UNLABELED_SET) \n",
        "        train_df = pd.read_csv(TRAIN_SET)\n",
        "        val_df = pd.read_csv(VALIDATION_SET)\n",
        "        test_df = pd.read_csv(TEST_SET)\n",
        "\n",
        "        pending_model = LSTM(num_classes=train_df[LABEL_COL].unique().shape[0])\n",
        "        optimal_model = None\n",
        "        \n",
        "        optimizer =  get_optimizer(pending_model.parameters())\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "        \n",
        "        print('start training loops. #epochs = ' + str(NUM_EPOCHS))\n",
        "        print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^11} | {'Test Acc':^9} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*50)  \n",
        "        \n",
        "        log_file.write(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^11} | {'Test Acc':^9} | {'Val Acc':^9} | {'Elapsed':^9}\\n\")\n",
        "        log_file.write(\"-\"*50 + \"\\n\")\n",
        "            \n",
        "        best_val_acc = 0\n",
        "        best_test_acc = 0\n",
        "        best_val_epoch = -1\n",
        "        best_test_epoch = -1\n",
        "        min_loss = 100\n",
        "        num_no_imp = 0\n",
        "        em_loss_func = EMLoss()\n",
        "        ml_loss_func = nn.CrossEntropyLoss()\n",
        "        validation_dl = get_data_loaders(val_df, pending_model.w2v_model, sampling_type=SEQUENTIAL_SAMPLING, is_labeled=1, break_df_func=break_by_batch_size)\n",
        "        test_dl = get_data_loaders(test_df, pending_model.w2v_model, sampling_type=SEQUENTIAL_SAMPLING,is_labeled=1, break_df_func=break_by_batch_size)\n",
        "        train_dl = get_data_loaders(train_df, pending_model.w2v_model, sampling_type=SEQUENTIAL_SAMPLING, is_labeled=1, break_df_func=break_by_batch_size)\n",
        "        non_labeled_dl = get_data_loaders(unlabeled_df, pending_model.w2v_model, sampling_type=SEQUENTIAL_SAMPLING, is_labeled=0, break_df_func=break_by_batch_size)\n",
        "        train_set_dl = train_dl + non_labeled_dl\n",
        "        print(\"type train set: \" + str(type(train_set_dl)))\n",
        "        print(\"len train set: \" + str(len(train_set_dl)))\n",
        "        print(\"len labeled train: \" + str(len(train_dl)))\n",
        "        print(\"len non labeled: \" + str(len(non_labeled_dl)))\n",
        "        for i in range(NUM_EPOCHS):\n",
        "            epoch = i + 1\n",
        "            epoch_start_time = time.time()\n",
        "            total_loss = 0\n",
        "            num_batches = 0\n",
        "\n",
        "            random.shuffle(train_set_dl)\n",
        "            pending_model.train()\n",
        "            for dl in train_set_dl:\n",
        "                for texts, labels, lengths, is_labeled_indicators in dl:\n",
        "                    texts = texts.to(active_device)\n",
        "                    labels = labels.to(active_device)\n",
        "                    lengths = lengths.to(torch.device(\"cpu\"))\n",
        "                    embed_texts = pending_model.embed_text(texts)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    ml_logits = pending_model(embed_texts, lengths)\n",
        "                    if is_labeled_indicators[0] > 0:\n",
        "                        ml_loss = ml_loss_func(ml_logits, labels)\n",
        "                    else:\n",
        "                        ml_loss = None\n",
        "\n",
        "                    x = embed_texts.clone().detach().to(torch.float)\n",
        "                    x = x.to(active_device)\n",
        "                    em_logits = pending_model(x, lengths)\n",
        "                    em_loss = em_loss_func(em_logits)\n",
        "\n",
        "                    #vat_loss = generate_vat_loss(embed_texts=embed_texts, lengths=lengths, model=pending_model)\n",
        "\n",
        "                    if ml_loss is not None:\n",
        "                        ml_loss.backward()\n",
        "                    em_loss.backward()\n",
        "                    #vat_loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                    total_loss += em_loss.item()\n",
        "                    #total_loss += vat_loss.item()\n",
        "                    if ml_loss is not None:\n",
        "                        total_loss += ml_loss.item()\n",
        "                    num_batches += 1\n",
        "\n",
        "            avg_loss = total_loss / num_batches\n",
        "            epoch_time = time.time() - epoch_start_time\n",
        "            \n",
        "            # Validation test.\n",
        "            val_acc, _ = test(pending_model, validation_dl)\n",
        "            train_acc, _ = test(pending_model, train_dl)\n",
        "            test_acc, _ = test(pending_model, test_dl)\n",
        "            val_acc *= 100\n",
        "            train_acc *= 100\n",
        "            test_acc *= 100\n",
        "            print(f\"{epoch:^7} | {avg_loss:^12.6f} | {train_acc:^9.2f} | {test_acc:^9.2f} |  {val_acc:^9.4f} | {epoch_time:^9.2f}\")\n",
        "            log_file.write(f\"{epoch:^7} | {avg_loss:^12.6f}  {train_acc:^9.2f} | {test_acc:^9.2f} |  {val_acc:^9.4f} | {epoch_time:^9.2f}\\n\")\n",
        "            log_file.flush()\n",
        "                  \n",
        "            if avg_loss < min_loss:\n",
        "                min_loss = avg_loss\n",
        "                num_no_imp = 0\n",
        "            else:\n",
        "                num_no_imp += 1\n",
        "                \n",
        "            if num_no_imp > MAX_NO_IMP and epoch > MIN_EPOCHS_TO_STOP:\n",
        "                print('early stop exit')\n",
        "                log_file.write('\\tEarly Stop exit\\n')\n",
        "                log_file.flush()\n",
        "                break\n",
        "            \n",
        "            if epoch < MIN_VALID_EPOCHS:\n",
        "                continue\n",
        "            \n",
        "            if avg_loss > MAX_VALID_LOSS:\n",
        "                continue\n",
        "            \n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                optimal_model = copy.deepcopy(pending_model)\n",
        "                best_val_epoch = epoch\n",
        "\n",
        "            if test_acc > best_test_acc:\n",
        "              best_test_acc = test_acc\n",
        "              best_test_epoch = epoch\n",
        "        \n",
        "        print('train_lstm_nlp - end')\n",
        "        print(\"Best test results: acc={:.3f}\".format(best_test_acc) + \", epoch=\" + str(best_test_epoch))\n",
        "        print(\"Best val results: acc={:.3f}\".format(best_val_acc) + \", epoch=\" + str(best_val_epoch))\n",
        "        log_file.write(\"Best test results: acc={:.3f}\".format(best_test_acc) + \", epoch=\" + str(best_test_epoch) + \"\\n\")\n",
        "        log_file.write(\"Best val results: acc={:.3f}\".format(best_val_acc) + \", epoch=\" + str(best_val_epoch) + \"\\n\")\n",
        "        log_file.flush()\n",
        "        end_train(pending_model, optimal_model, test_dl, validation_dl, log_file)\n",
        "        return pending_model, optimal_model, best_val_epoch\n"
      ],
      "metadata": {
        "id": "4oDLWsAhbEiQ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer()\n",
        "last_model, opt_model, best_epoch = trainer.train()\n",
        "print(best_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pjAbYmEf3lF",
        "outputId": "44b99973-7fa0-440e-88d7-995a19848b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lstm trainer - start\n",
            "Freeze embedding matrix = True\n",
            "load embedding model\n",
            "drive/MyDrive/ColabData/embed.model\n",
            "85899\n",
            "85898 256\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 85898/85898 [00:08<00:00, 10013.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\tw2v after padding: (85900, 256)\n",
            "generate embedding tensor\n",
            "start training loops. #epochs = 200\n",
            " Epoch  |  Train Loss  |  Train Acc  | Test Acc  |  Val Acc  |  Elapsed \n",
            "--------------------------------------------------\n",
            "type train set: <class 'list'>\n",
            "len train set: 3802\n",
            "len labeled train: 1869\n",
            "len non labeled: 1933\n",
            "   1    |   0.604605   |   85.38   |   85.12   |   83.3727  |  340.49  \n",
            "   2    |   0.388632   |   88.23   |   88.15   |   87.9433  |  339.92  \n",
            "   3    |   0.345055   |   88.86   |   88.27   |   87.1552  |  340.41  \n",
            "   4    |   0.322034   |   90.09   |   89.34   |   88.0221  |  340.18  \n",
            "   5    |   0.305352   |   91.06   |   90.04   |   88.3373  |  340.30  \n",
            "   6    |   0.292830   |   90.81   |   89.94   |   89.1253  |  340.52  \n",
            "   7    |   0.269239   |   91.60   |   90.33   |   90.3861  |  340.58  \n",
            "   8    |   0.260764   |   91.43   |   90.12   |   88.9677  |  339.87  \n",
            "   9    |   0.250274   |   93.30   |   91.48   |   90.7801  |  340.16  \n",
            "  10    |   0.242584   |   93.65   |   91.62   |   91.4894  |  340.50  \n",
            "  11    |   0.227677   |   93.62   |   91.44   |   90.5437  |  340.04  \n",
            "  12    |   0.222426   |   94.50   |   91.78   |   90.9377  |  340.06  \n",
            "  13    |   0.214856   |   95.22   |   92.04   |   90.7013  |  340.12  \n",
            "  14    |   0.203997   |   95.20   |   92.00   |   91.4106  |  339.90  \n",
            "  15    |   0.204013   |   94.71   |   91.64   |   90.7013  |  339.82  \n",
            "  16    |   0.199362   |   95.47   |   91.84   |   90.7801  |  340.58  \n",
            "  17    |   0.184501   |   95.35   |   91.36   |   91.0954  |  340.13  \n",
            "  18    |   0.173655   |   96.54   |   92.08   |   91.4106  |  340.31  \n",
            "  19    |   0.165427   |   95.84   |   91.44   |   90.9377  |  340.53  \n",
            "  20    |   0.158569   |   96.63   |   92.09   |   90.7013  |  339.83  \n",
            "  21    |   0.152437   |   96.54   |   91.83   |   90.7013  |  340.09  \n",
            "  22    |   0.148369   |   96.08   |   91.45   |   90.6225  |  340.19  \n",
            "  23    |   0.146954   |   96.36   |   91.36   |   89.7557  |  340.07  \n",
            "  24    |   0.138844   |   97.35   |   91.98   |   90.9377  |  340.38  \n",
            "  25    |   0.134502   |   97.65   |   92.17   |   91.0165  |  340.33  \n",
            "  26    |   0.127932   |   97.76   |   92.13   |   91.7258  |  340.22  \n",
            "  27    |   0.125775   |   97.90   |   92.24   |   91.5682  |  340.11  \n",
            "  28    |   0.122718   |   98.13   |   92.24   |   90.6225  |  340.29  \n",
            "  29    |   0.118520   |   98.04   |   92.14   |   91.3318  |  340.47  \n",
            "  30    |   0.115309   |   98.36   |   92.21   |   91.3318  |  339.71  \n",
            "  31    |   0.116531   |   98.66   |   92.27   |   91.5682  |  340.13  \n",
            "  32    |   0.109173   |   98.50   |   92.28   |   91.3318  |  340.02  \n",
            "  33    |   0.103879   |   98.70   |   92.26   |   91.0165  |  339.81  \n",
            "  34    |   0.099250   |   97.96   |   91.51   |   89.6769  |  339.86  \n",
            "  35    |   0.104482   |   98.84   |   92.18   |   90.5437  |  340.07  \n",
            "  36    |   0.096151   |   98.62   |   92.08   |   91.3318  |  340.29  \n",
            "  37    |   0.094291   |   98.81   |   92.12   |   91.7258  |  340.55  \n",
            "  38    |   0.094345   |   99.04   |   92.19   |   91.4894  |  340.59  \n",
            "  39    |   0.097162   |   91.83   |   86.92   |   86.8400  |  340.35  \n",
            "  40    |   0.143365   |   97.75   |   91.15   |   89.6769  |  339.78  \n",
            "  41    |   0.113232   |   98.94   |   92.10   |   91.3318  |  340.22  \n",
            "  42    |   0.093428   |   99.13   |   92.26   |   91.3318  |  340.25  \n",
            "  43    |   0.083344   |   98.66   |   91.86   |   90.7013  |  340.57  \n",
            "  44    |   0.087282   |   98.85   |   91.93   |   90.7013  |  340.13  \n",
            "  45    |   0.080530   |   99.30   |   92.36   |   91.1742  |  339.85  \n",
            "  46    |   0.081222   |   99.25   |   92.34   |   90.7801  |  340.21  \n",
            "  47    |   0.079968   |   99.04   |   91.90   |   90.9377  |  340.70  \n",
            "  48    |   0.079378   |   99.38   |   92.28   |   90.3073  |  340.15  \n",
            "  49    |   0.073596   |   98.95   |   91.94   |   90.5437  |  340.28  \n",
            "  50    |   0.077616   |   99.32   |   92.21   |   91.1742  |  340.45  \n",
            "  51    |   0.076424   |   99.14   |   91.91   |   90.7801  |  340.08  \n",
            "  52    |   0.071363   |   99.11   |   92.04   |   90.7013  |  339.50  \n"
          ]
        }
      ]
    }
  ]
}