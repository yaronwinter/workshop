{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df6baa4-ac10-498f-86c1-66ac8b00973e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_evaluation_batchsize_n_epoch_effect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiment_evaluation_batchsize_n_epoch_effect.py\n",
    "from segmenter.using_whisper.evaluation_on_dataset import get_whisper_tagging_on_dataset\n",
    "from segmenter.using_whisper.model_inference import get_model\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "\n",
    "def get_evaluation_on_testset(model_path,model_arc):\n",
    "    \"\"\"\n",
    "    loads the model and the 5% testset dataset. \n",
    "    Then applys whisper evaluation on the dataset \n",
    "    and gets the kappa values for each metric. \n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = get_model(\n",
    "        device=device,\n",
    "        model_path=model_path,\n",
    "        model_arc=model_arc,\n",
    "    )\n",
    "    ds = datasets.DatasetDict.load_from_disk(\"/home/moshebr/dharelg/moshe/ds_eran_original/\")[\"test\"]\n",
    "    df_eval_for_ds_ = get_whisper_tagging_on_dataset(ds,model)\n",
    "\n",
    "    rr=df_eval_for_ds_\n",
    "    rr_wo_dups = rr[rr.dataset_id.duplicated()]\n",
    "    return pd.Series({\n",
    "        \"segmentation_kappa\":metrics.cohen_kappa_score(rr.segmentation_truth,rr.segmentation_pred),\n",
    "        \"segmentation_kappa_wo_start_of_turns\":metrics.cohen_kappa_score(rr_wo_dups.segmentation_truth,rr_wo_dups.segmentation_pred),\n",
    "        \"focus_kappa\":metrics.cohen_kappa_score(rr.emphasis_truth,rr.emphasis_pred),\n",
    "        \"prototype_kappa\":metrics.cohen_kappa_score(rr.prototype_truth,rr.prototype_pred),\n",
    "        \"model_path\":model_path,\n",
    "        \"model_arc\":model_arc,\n",
    "    })\n",
    "\n",
    "\n",
    "samples2 = pd.DataFrame([ # model_path,model_arc, desc\n",
    "(\n",
    "    f\"/home/moshebr/dharelg/moshe/experiment_8_batchsize10_modelarcsmall_labelsreg/checkpoint-{i}00/pytorch_model.bin\",\n",
    "    \"small\",\n",
    "    f\"smal batch 10, {i}00 steps (1 epoch) token-per-tag\",\n",
    ") for i in range(1,11)\n",
    "], columns = [\"model_path\",\"model_arch\",\"model_description\"])\n",
    "# samples2 = pd.DataFrame([ # model_path,model_arc, desc\n",
    "#     (\n",
    "#         \"/home/moshebr/dharelg/moshe/experiment_2_batchsize10_modelarcmedium/checkpoint-100/pytorch_model.bin\",\n",
    "#         \"medium\",\n",
    "#         \"medium batch 10, 100 steps (1 epoch)\",\n",
    "#     ),(\n",
    "#         \"/home/moshebr/dharelg/moshe/experiment_2_batchsize10_modelarcmedium/checkpoint-300/pytorch_model.bin\",\n",
    "#         \"medium\",\n",
    "#         \"medium batch 10, 300 steps (3 epochs)\",\n",
    "#     ),(\n",
    "#         \"/home/moshebr/dharelg/moshe/experiment_2_batchsize10_modelarcmedium/checkpoint-600/pytorch_model.bin\",\n",
    "#         \"medium\",\n",
    "#         \"medium batch 10, 600 steps (6 epochs)\",\n",
    "#     ),(\n",
    "#         '/home/moshebr/dharelg/moshe/experiment_1_batchsize10/checkpoint-300/pytorch_model.bin',\n",
    "#         \"small\",\n",
    "#         \"small no accumulation, batch size 10, 300 steps\",\n",
    "#     ),(\n",
    "#         '/home/moshebr/dharelg/moshe/recreation_script_again_w_small/checkpoint-1000/pytorch_model.bin',\n",
    "#         \"small\",\n",
    "#         \"original train small (accumulation 16 and batch size 1 with 1000 steps)\",\n",
    "#     ),(\n",
    "#         '/home/moshebr/dharelg/moshe/recreation_script_again_w_medium/checkpoint-1000/pytorch_model.bin',\n",
    "#         \"medium\",\n",
    "#         \"original train medium (accumulation 16 and batch size 1 with 1000 steps)\",\n",
    "#     ),\n",
    "# ], columns = [\"model_path\",\"model_arch\",\"model_description\"])\n",
    "# samples2.to_csv(\"experiment_documentation_with_probably_some_errors2.csv\")\n",
    "\n",
    "aaa=samples2.apply(lambda x:get_evaluation_on_testset(x.model_path,x.model_arch),axis=1)\n",
    "\n",
    "# aaa.to_csv(\"/home/moshebr/some_evaluations_test_set2_update.csv\")\n",
    "\n",
    "# print(aaa)\n",
    "# aaa\n",
    "\n",
    "import pandas as pd \n",
    "a=pd.merge(\n",
    "  samples2,\n",
    "  aaa\n",
    "    ).drop(columns=[\"model_arc\"])\n",
    "a.to_csv(\"small_labels_verions_reg_eval.csv\")\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
    "#     display(a.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3f84f75-43ff-4904-8e6b-c6c380c2c7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting experiment_evaluation_batchsize_n_epoch_effect.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile experiment_evaluation_batchsize_n_epoch_effect.sh\n",
    "sbatch <<EOT\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=booya # Job name\n",
    "#SBATCH --mem=60gb # Job memory request\n",
    "#SBATCH --partition=normal.q # partition request\n",
    "#SBATCH --gres=gpu:a10:1 # GPU devices request\n",
    "#SBATCH --output=/home/moshebr/notebooks/lets_do_whisper/experiment_evaluation_batchsize_n_epoch_effect.log # Standard output and error log\n",
    "#SBATCH --time=3:00:00 # Time limitation \n",
    "#SBATCH --mail-user=barboym@yahoo.com # Email address for notification sending\n",
    "#SBATCH --mail-type=END,FAIL # When to send email notification\n",
    "/home/moshebr/.conda/envs/whisper/bin/python /home/moshebr/notebooks/lets_do_whisper/experiment_evaluation_batchsize_n_epoch_effect.py ${1}\n",
    "EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7affd5e2-996c-4657-b467-e458e755043a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 33923\n"
     ]
    }
   ],
   "source": [
    "!bash experiment_evaluation_batchsize_n_epoch_effect.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b95d66-49d0-406a-adcc-5dcc912491cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON) \n",
      "             33909  normal.q     bash    shify  R    5:46:29      1 n115 \n",
      "             33922  normal.q     bash  moshebr  R      16:18      1 n115 \n",
      "             33910  normal.q     bash zacharyl  R    5:45:23      1 n114 \n",
      "             33903    long.q     bash   rodfon  R    9:28:59      1 n114 \n",
      "             33905    long.q     bash     liav  R    8:11:17      1 n114 \n",
      "             33907    long.q       jk    guylu  R    7:31:48      1 n114 \n"
     ]
    }
   ],
   "source": [
    "!squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99f8441c-10fb-424a-8ae3-c6297de94eef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening a new Slurm session:\n",
      "==========================================\n",
      "SLURM_JOB_ID = 33835\n",
      "SLURM_NODELIST = n131\n",
      "------------------------------------------\n",
      "Personal temporary local directory allocated: $TMPDIR=/local_data/33835_moshebr/\n",
      "Your free local storage on your $TMPDIR is currently:  695G\n",
      "You can use variable name $TMPDIR in your session\n",
      "==========================================\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "cat /home/moshebr/notebooks/lets_do_whisper/experiment_evaluation_batchsize_n_epoch_effect.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fa6c7a5c-ca32-433b-a77c-ddfb2f670dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_description</th>\n",
       "      <th>segmentation_kappa</th>\n",
       "      <th>segmentation_kappa_wo_start_of_turns</th>\n",
       "      <th>focus_kappa</th>\n",
       "      <th>prototype_kappa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>medium batch 10, 100 steps (1 epoch)</td>\n",
       "      <td>0.925409</td>\n",
       "      <td>0.906374</td>\n",
       "      <td>0.295411</td>\n",
       "      <td>0.011703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>medium batch 10, 300 steps (3 epochs)</td>\n",
       "      <td>0.937554</td>\n",
       "      <td>0.920190</td>\n",
       "      <td>0.400008</td>\n",
       "      <td>0.502257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>medium batch 10, 600 steps (6 epochs)</td>\n",
       "      <td>0.927831</td>\n",
       "      <td>0.907225</td>\n",
       "      <td>0.474813</td>\n",
       "      <td>0.580043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>small no accumulation, batch size 10, 300 steps</td>\n",
       "      <td>0.922183</td>\n",
       "      <td>0.902103</td>\n",
       "      <td>0.423535</td>\n",
       "      <td>0.353964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>original train small (accumulation 16 and batc...</td>\n",
       "      <td>0.891695</td>\n",
       "      <td>0.862690</td>\n",
       "      <td>0.511850</td>\n",
       "      <td>0.362958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>original train medium (accumulation 16 and bat...</td>\n",
       "      <td>0.938912</td>\n",
       "      <td>0.922793</td>\n",
       "      <td>0.496077</td>\n",
       "      <td>0.526133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   model_description  segmentation_kappa   \n",
       "0               medium batch 10, 100 steps (1 epoch)            0.925409  \\\n",
       "1              medium batch 10, 300 steps (3 epochs)            0.937554   \n",
       "2              medium batch 10, 600 steps (6 epochs)            0.927831   \n",
       "3    small no accumulation, batch size 10, 300 steps            0.922183   \n",
       "4  original train small (accumulation 16 and batc...            0.891695   \n",
       "5  original train medium (accumulation 16 and bat...            0.938912   \n",
       "\n",
       "   segmentation_kappa_wo_start_of_turns  focus_kappa  prototype_kappa  \n",
       "0                              0.906374     0.295411         0.011703  \n",
       "1                              0.920190     0.400008         0.502257  \n",
       "2                              0.907225     0.474813         0.580043  \n",
       "3                              0.902103     0.423535         0.353964  \n",
       "4                              0.862690     0.511850         0.362958  \n",
       "5                              0.922793     0.496077         0.526133  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "a=pd.merge(\n",
    "  samples2,\n",
    "  aaa\n",
    "    ).drop(columns=[\"model_arc\"])\n",
    "a.to_csv(\"small_labels_verions_reg_eval\")\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):\n",
    "    display(a.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1533d-6ba5-4955-ba5d-617d83adb42b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
