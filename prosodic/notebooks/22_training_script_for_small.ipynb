{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce700aea-6c74-43bb-9c9d-1d4a7ca39ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "from evaluate.evaluator import text2text_generation\n",
    "from transformers import pipeline\n",
    "from evaluate.evaluator import text2text_generation\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import Dataset, DatasetDict\n",
    "import transformers\n",
    "import torch\n",
    "import evaluate\n",
    "import sys \n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# Load the list from the file\n",
    "with open(\"/home/yadinb/dharelg/yadinb/alona_tag_data_POC.pkl\", 'rb') as f:\n",
    "    df = pd.DataFrame(pickle.load(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7534c18b-6224-4cee-a802-e88bf81ba1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'path': '/home/eranbe/dharelg/datasets/whisper_dataset/prototype_dataset/TAL_4prototype_tmpfiles_4creating_dataset/wav/0.wav',\n",
       " 'array': array([-0.07229415, -0.10425258, -0.08992703, ..., -0.01851533,\n",
       "        -0.03886626,  0.        ]),\n",
       " 'sampling_rate': 16000}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0].audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7005e08-5072-40cd-b14b-3557c9056239",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_script_working.py\n",
    "from transformers import WhisperProcessor\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "from evaluate.evaluator import text2text_generation\n",
    "from transformers import pipeline\n",
    "from evaluate.evaluator import text2text_generation\n",
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from datasets import Dataset, DatasetDict\n",
    "import transformers\n",
    "import torch\n",
    "import evaluate\n",
    "import sys \n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "# Load the list from the file\n",
    "with open(\"/home/yadinb/dharelg/yadinb/alona_tag_data_POC.pkl\", 'rb') as f:\n",
    "    df = pd.DataFrame(pickle.load(f))\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    row['audio']['array'] = np.array(row['audio']['array'], dtype=np.float32)\n",
    "\n",
    "    \n",
    "model_arch = \"small\"\n",
    "print(\"model_arch:\",model_arch)\n",
    "output_dir = f\"/home/moshebr/dharelg/moshe/recreation_script_again_w_{model_arch}\"\n",
    "\n",
    "seed = 42  # Or any other number you like\n",
    "\n",
    "train_df = df.iloc[:int(0.95*len(df))]\n",
    "train_df = train_df.sample(frac=1, random_state=seed).reset_index(drop=True)  # shuffle and reset index\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "\n",
    "\n",
    "# Create the Dataset objects\n",
    "#train_dataset = Dataset.from_pandas(df.iloc[:int(0.95*len(df))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df.iloc[int(0.95*len(df)):])\n",
    "\n",
    "# Create the DatasetDict object\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "TEST_DF = df.iloc[int(0.95*len(df)):]\n",
    "feature_extractor = transformers.WhisperFeatureExtractor.from_pretrained(f\"openai/whisper-{model_arch}\")\n",
    "\n",
    "tokenizer = transformers.WhisperTokenizer.from_pretrained(f\"openai/whisper-{model_arch}\", language=\"english\", task=\"transcribe\")\n",
    "\n",
    "processor = transformers.WhisperProcessor.from_pretrained(f\"openai/whisper-{model_arch}\", language=\"english\", task=\"transcribe\")\n",
    "#need to trancate sequence ,model works only with 1024 token.\n",
    "#@@@@@@@@@@\n",
    "#@@@@@@@@@@\n",
    "#$$$$$$$$$$$$$$$$$\n",
    "#MAX_LENGTH = 1024\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array\n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # truncate target text to a maximum length\n",
    "    #sentence = batch[\"sentence\"][:MAX_LENGTH]\n",
    "    sentence = batch[\"sentence\"]\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # split the string into list of words\n",
    "    words = sentence.split()\n",
    "\n",
    "    # define the markers\n",
    "    markers = ['TRUE1TRUE', 'FALSE1TRUE', 'TRUE2TRUE', 'FALSE2TRUE', 'TRUE3TRUE', 'FALSE3TRUE', 'TRUE1FALSE', 'FALSE1FALSE', 'TRUE2FALSE', 'FALSE2FALSE', 'TRUE3FALSE', 'FALSE3FALSE']\n",
    "\n",
    "    # process the words\n",
    "    output_words = [word.lower() if word not in markers else word for word in words]\n",
    "\n",
    "    # join the words back into a string\n",
    "    sentence = ' '.join(output_words)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # compute input length of audio sample in seconds\n",
    "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "    # encode target text to label ids\n",
    "    labels_ids = tokenizer(sentence).input_ids\n",
    "    # truncate labels to a maximum length\n",
    "    #labels_ids = labels_ids[:MAX_LENGTH]\n",
    "    batch[\"labels\"] = labels_ids\n",
    "    return batch\n",
    "dataset = dataset_dict.map(prepare_dataset, remove_columns=dataset_dict.column_names[\"train\"], num_proc=8)\n",
    "max_input_length = 30\n",
    "min_input_length = 0\n",
    "\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length > min_input_length and length <= max_input_length\n",
    "vectorized_datasets = dataset.filter(\n",
    "    is_audio_in_length_range, num_proc=1, input_columns=[\"input_length\"]\n",
    ")\n",
    "\n",
    "model = transformers.WhisperForConditionalGeneration.from_pretrained(f\"openai/whisper-{model_arch}\")\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n",
    "model.config.suppress_tokens = []\n",
    "max_label_length = model.config.max_length\n",
    "def is_labels_in_length_range(labels):\n",
    "    return len(labels) < max_label_length\n",
    "common_voice = vectorized_datasets.filter(\n",
    "    is_labels_in_length_range, num_proc=1, input_columns=[\"labels\"]\n",
    ")\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "training_args = transformers.Seq2SeqTrainingArguments(\n",
    "    output_dir =output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=1000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=448,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"none\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=common_voice[\"train\"],\n",
    "    eval_dataset=common_voice[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "trainer.train()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
