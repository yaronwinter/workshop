{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "07ebe68e-4f2e-44aa-b8bc-bb9014502783",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_script_for_yadin.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_script_for_yadin.py\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "import transformers\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import datasets\n",
    "import pydub\n",
    "import whisper\n",
    "    \n",
    "model_arch = \"tiny\" # TODO: maybe change to small for actual checks \n",
    "print(\"model_arch:\",model_arch)\n",
    "output_dir = f\"/home/moshebr/dharelg/moshe/expeimental_model_SBC_{model_arch}\"\n",
    "\n",
    "seed = 42  # Or any other number you like\n",
    "\n",
    "processor = transformers.WhisperProcessor.from_pretrained(f\"openai/whisper-{model_arch}\", language=\"english\", task=\"transcribe\")\n",
    "feature_extractor = processor.feature_extractor\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "model = transformers.WhisperForConditionalGeneration.from_pretrained(f\"openai/whisper-{model_arch}\")\n",
    "model.config.forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"english\", task=\"transcribe\")\n",
    "model.config.suppress_tokens = []\n",
    "\n",
    "# Thats the DataSet You worked on, right? \n",
    "ds=datasets.Dataset.load_from_disk(\"/home/yadinb/dharelg/ds_for_yadin_SBC_wo_augmentation/\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Train test split \n",
    "train_fraction = 0.8\n",
    "ds_indices = list(range(len(ds)))\n",
    "train_ids = ds_indices[:int(len(ds)*train_fraction)]\n",
    "test_ids = ds_indices[int(len(ds)*train_fraction):]\n",
    "ds_train = ds.select(train_ids)\n",
    "ds_test = ds.select(test_ids)\n",
    "\n",
    "\n",
    "## Getting the input_features from the wav (Be vary of this method. make sure this doesn't produce a bad model!!!!)\n",
    "def load_input_features_from_path(audio_path):\n",
    "    \"\"\"\n",
    "    retrieves mel from wav \n",
    "    \"\"\"\n",
    "    # audio = pydub.AudioSegment.from_file(audio_path).get_array_of_samples() # doesnt work\n",
    "    audio = whisper.audio.load_audio(audio_path) # seems to work (based on mmfpeg)\n",
    "    mel_from_audio_path = feature_extractor(\n",
    "        audio,\n",
    "        sampling_rate=16000,\n",
    "    )[\"input_features\"]\n",
    "    return mel_from_audio_path\n",
    "\n",
    "# the new dataset have a field based on the wav file\n",
    "ds_from_path=ds.map(lambda x:{\"input_features_from_path\":load_input_features_from_path(x[\"path\"])},num_proc=8)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "training_args = transformers.Seq2SeqTrainingArguments(\n",
    "    output_dir =output_dir,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=1000,\n",
    "    gradient_checkpointing=True, # trick to conserve GPU-memory (longer training time in order to fit in memory)\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=448,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"none\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_test,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n",
    "processor.save_pretrained(training_args.output_dir)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08345ef4-bd33-45d0-b996-f6ea3c57ebe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(audio_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
